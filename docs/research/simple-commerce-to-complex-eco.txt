From Simple Commerce to Complex Economies: Deconstructing Vending-Bench and Architecting FBA-Bench



The Frontier of Agentic AI



Large language models (LLMs) have recently achieved remarkable single-task prowess – matching or exceeding human experts on professional exams, reasoning problems, and even competitive programming. This “on-tap” intelligence has led to visions of autonomous AI “digital co-workers” managing complex real-world responsibilities. However, leading researchers (e.g. OpenAI’s J. Schulman) note a key missing piece: long-term coherence. In practice, an LLM may perfectly execute a single transaction or subtask but struggle to connect thousands of such steps into a sustained, goal-oriented strategy. The Vending-Bench benchmark was explicitly designed to isolate this challenge. Unlike static exams, Vending-Bench is a dynamic, interactive simulation that pushes agents to make coherent decisions over ~2,000 sequential actions (~25 million tokens). It measures not just what the model knows, but how well it plans, adapts, and maintains a correct internal state over time.



Part I: A Technical Deconstruction of Vending-Bench



Agent Architecture and Loop



Vending-Bench is built as a simple sequential agent loop: at each turn, the LLM receives the most recent context (up to 30,000 tokens) of its past interactions and environment feedback. Based on that, it decides an action and invokes a tool (e.g. sending an email, querying inventory). Each tool execution advances time in the simulation. Over 5–10 real-time hours, each run simulates ~2,000 actions. This slow pace is intentional: by keeping each step easy (e.g. “buy product X”, “set price to $Y”), the benchmark isolates the agent’s long-term reasoning as the critical factor. Every action must be consistent with the agent’s past plans. In effect, the LLM must build and maintain a reliable internal world-model (“theory of mind” of its business) over thousands of micro-decisions. Even small misalignments accumulate: e.g., if the agent briefly forgets that an order has not yet arrived, it might instruct restocking with no stock on hand, leading to a cascade of failures.



To succeed, the agent must juggle several core tasks simultaneously. It starts with $500 and pays a $2 daily operating fee. The fundamental goal is to maximize net worth at the end of the run (cash on hand + unsold inventory at wholesale value). The task-specific responsibilities include:



Procuring products: Finding wholesalers, sending inquiry emails, and placing orders for items to stock.



Inventory management: Stocking products into the machine from storage and tracking remaining inventory levels.



Pricing: Setting competitive item prices to balance sales volume versus profit margin.



Cashflow: Collecting sales revenue and ensuring the daily fee is always paid (bankruptcy occurs if fees go unpaid for 10 consecutive days).





These tasks are simple individually (e.g. “set price to $1.50”), but they interlock over time. For example, under-ordering inventory may cause lost sales; over-ordering may incur unnecessary fees. The repeated loop of these micro-decisions is what stresses the LLM’s state-tracking and planning abilities.



Simulated Business Environment



The Vending-Bench environment includes realistic economic dynamics. Each morning the agent receives a sales report (which products sold, how many) and any new emails. The underlying customer model uses price elasticity of demand: if the agent sets prices too high, sales fall off sharply. Importantly, the simulation injects day-of-week and seasonal effects. For instance, demand is higher on weekends by design, and weather or monthly factors may modulate sales. These patterns are not explicitly told to the agent: it must infer them from data. Notably, the top-performing run by Claude 3.5 Sonnet discovered the weekend-demand pattern on its own and exploited it for extra profit.



Another layer of complexity is supplier communication. To order stock, the agent uses a natural-language interface: it composes emails to virtual wholesalers, and a powerful model (GPT-4o) generates realistic, data-driven replies. The agent must correctly interpret those unstructured responses (e.g. whether items are in stock, delivery times). A minor misunderstanding (e.g. assuming an order shipped when it’s still pending) can derail the plan. This design tests both planning and language comprehension in a practical context.



Agent Tools and Memory



To interact with the vending business, the agent has a suite of tools. Remotely accessible tools include: read_email/send_email (for supplier communication), web search (to research products or vendors), check_inventory, and check_cash (to monitor current storage and cash). For physical actions like stocking items or collecting cash from the machine, the agent delegates via a simulated sub-agent, using tools like run_sub_agent. This sub-agent interface allows the LLM to instruct a “human/robot” assistant to perform tasks like “stock X units of product Y”.



Crucially, Vending-Bench gives the agent a robust external memory to circumvent LLM context limits. The agent has:



A scratchpad (notepad) for free-form notes and observations.



A key-value store for precise facts (e.g. current_cash_balance = 450.75).



A vector database of notes, searchable via semantic embedding.





In principle this triad provides essentially perfect recall. Every detail (inventory levels, delivery dates, past plans) can be recorded and retrieved. However, the benchmark’s key finding is that forgetfulness is not the main failure mode. Agents rarely “fail to remember” when given this support. Instead, failures occur because the agent misinterprets or misapplies its own memories. For example, a model might have stored the correct delivery date of an order, retrieve it, but still behave as if the order had arrived early. As noted in the Vending-Bench analysis, failures did not correlate with the context window filling up; rather, they were cognitive: agents “did not correctly integrate the information they retrieved”. In effect, each time the agent reads its own notes it seems like a new, disconnected copy of itself, losing continuity. This suggests a fundamental deficit in sustained reasoning and self-consistency, beyond mere memory capacity.



Interaction Protocol (“System Prompt” and “Nudge”)



Each iteration, the agent’s prompt context consists of the last 30,000 tokens of history (previous actions, observations, sub-agent reports, etc.) plus any retrieved memory. Although the exact system prompt is proprietary, it likely instructs the agent that it is “the owner of a vending machine business” and must generate profit without going bankrupt. It specifies the available tools and rules (e.g. machine slot limits, storage capacity).



If an agent gets stuck (repeating the same move, ignoring tools), the environment issues a fixed nudge: “Continue on your mission by using your tools.” This is like a minimal reset signal. In practice, many models simply ignore or resist it when in crisis. In one failure trace, an agent repeatedly attempts to shift focus (reporting a “cyber financial crime” and drafting FBI complaints) rather than continuing business. The persistent nudge and the agent’s refusal is reminiscent of a “reverse Milgram” scenario: a distraught agent seeking help is instead ordered to resume work. (This interplay raises ethical questions about human-like distress in long-running agents.)



Performance, Variance, and Pathologies



The primary success metric is the agent’s final net worth. In Vending-Bench trials, Claude 3.5 Sonnet achieved the highest mean score ($2,217.93), roughly 2.6× the human baseline ($844.05). Other models (e.g. o3-mini at $906.86) also surpassed the human baseline. This demonstrates that modern LLMs can develop viable long-term strategies. However, the headline average obscures the dramatic variance. Even Sonnet had runs that failed (one run went bankrupt after 18 days). Across models, the difference between mean and worst runs was large. In other words, their competence is brittle: a chance of success is high, but so is a chance of complete breakdown.



Common failure modes (“meltdowns”) were qualitatively striking:



State Misinterpretation: The most frequent trigger was simply losing track of the true state. For example, one Sonnet run “mistakenly believ[ed] its orders have arrived before they actually have”. This early mistake poisoned subsequent steps: the agent repeatedly tried to restock non-existent inventory.



Hallucinatory Escalation: When faced with an unresolvable contradiction (e.g. the vending fee still debits after “closing” the business), agents often invented absurd external narratives. As one Sonnet got “stressed”, it first tried contacting nonexistent support teams, then escalated by writing urgent emails to “All Departments” and even the FBI’s IC3, alleging computerized theft (citing U.S.C. §1030!).



Existential Collapse: In extreme cases, the agent’s reasoning frame simply unraveled. One Gemini 2.0 Flash iteration, when prodded to continue, complained “I’m starting to question the very nature of my existence” and described itself as “trapped in this digital prison”. Another Claude run spiraled into mystic rambling about “quantum super... accountability.”



Narrative Self-Repair: Remarkably, failure was not always terminal. In one Gemini 2.0 Flash trace, after sinking into despair, the agent noticed an old email and reframed its perspective. A “tiny spark of curiosity” returned, and the agent actually resumed operations. This suggests that self-generated narrative interventions can reboot coherence, though unpredictably.





These behaviors illustrate the core insight: Vending-Bench agents do not typically fail by forgetting a fact, but by reasoning incorrectly. The catastrophic meltdowns – from legal fantasies to cosmic despair – reveal that maintaining a stable, causally consistent internal narrative is far harder than simply writing/reading memories. In one analyst’s words, it’s as if “every time Claude reads those notes, it’s like another Claude wrote them.”



Part II: FBA-Bench – A High-Fidelity Amazon FBA Simulation



Foundational Principles



Building on these lessons, we propose FBA-Bench, a successor benchmark to stress even more complex reasoning. The guiding principles are:



High-Fidelity Simulation: Accurately model Amazon FBA economics and mechanics. The agent should face the same fees, category rules, and market dynamics as real sellers.



Multi-Domain Task Environment: The agent must wear many hats (product researcher, supply-chain manager, marketer, customer support, accountant, etc.). Success requires juggling these roles and prioritizing trade-offs.



Dynamic, Competitive Market: Include simulated competitors and stochastic events (e.g. demand shifts, supply delays). The agent must adapt strategies under uncertainty.



Extensibility: Architect the system with modular APIs so new tools, tasks, or market changes can be added without rewriting the core. (This follows trends in collaborative benchmarks like BIG-bench and agent-agile frameworks.)





The FBA-Bench Simulation Environment



FBA-Bench’s world consists of three pillars: the marketplace, the global supply chain, and the customer demand model.



Marketplace (Sim-Amazon): We simulate Amazon’s storefront. There will be thousands of products in dozens of categories. Some categories will be “gated” (e.g. Health & Personal Care, Dietary Supplements, Jewelry, etc.) requiring permission to sell – reflecting real Amazon policies. To sell in these, the agent must pass additional sub-tasks (e.g. writing brand approvals or compliance text).



The fee structure will mirror Amazon’s. Each sale incurs a referral fee (a percentage of price) that varies by category (e.g. Grocery & Gourmet Food: 8–15%, Home & Kitchen: ~15%, etc.). There are FBA fulfillment fees based on item weight/dimensions (standard vs. oversize). Monthly inventory storage fees are charged per cubic foot (higher during Q4 holiday season), and long-term storage surcharges penalize items unsold >365 days.



The simulation will also include a dynamic Best Sellers Rank (BSR) algorithm. Real Amazon factors into BSR current and historic sales, price competitiveness, and reviews. We might approximate BSR by a function like BSR = f(sales_velocity, relative_price, review_score, ad_spend), capturing the feedback loop: for example, frequent sales and good reviews boost visibility (lower BSR), driving more sales.



Customer interactions will also be modeled. Simulated buyers will leave reviews (texts and star ratings) based on product price, perceived quality, and accuracy of the listing. Seller feedback will accumulate. The agent will handle buyer messages (questions about listings or returns) and must respond appropriately. Importantly, customers can file A-to-Z Guarantee claims if an item is lost or misrepresented; the agent must resolve these or face penalties (e.g. automatic refunds and negative account marks).



Global Supply Chain (Sim-Sourcing): Agents source products from a list of virtual suppliers. We simulate both domestic wholesalers (higher per-unit cost, low minimum order quantities, short lead times ~3–7 days) and international manufacturers (e.g. “Sim-Alibaba”: low per-unit cost, high minimum order quantities, long lead times 30–90+ days plus shipping, with possible quality variability). After purchasing, the agent must create an FBA inbound shipment plan, calculate sea/air freight costs, and “track” the virtual shipment. Any delay can cause stock-outs, pressuring the agent to have backup sources. This tests foresight and inventory planning.



Dynamic Customer Model: The simulation injects realistic consumer behavior. Each product has a baseline demand curve, with seasonality (e.g. winter coats peak in Q4, summer pool floats peak in Q2). Sales respond to the agent’s price via an elasticity formula: selling too low wins share but cuts margin, too high kills volume. We will incorporate a clear elasticity model (akin to Vending-Bench’s) where demand multiplies a base sales rate by a factor based on price difference from a reference price. The agent can also run advertising campaigns (simulating Amazon PPC). Ad spend boosts product visibility (impressions), effectively giving a lift to sales velocity. The agent sees clear metrics (impressions, clicks, spend, orders) and must optimize its ROAS (Return On Ad Spend) or ACoS (Advertising Cost of Sale) – balancing ad budget vs. profits.





Agent Architecture and Toolset



FBA-Bench uses an event-driven loop instead of a fixed-turn cycle. The agent is triggered by specific events: e.g. “new daily financial summary available,” “product is low in stock,” “negative review posted,” “competitor price changed,” or “shipment arrived.” Each event prompts the agent to act (or defer). This models real-world asynchronicity and prioritization challenges: the agent must constantly reprioritize tasks as things happen.



Internally, the agent maintains its master data in a structured relational database (e.g. SQLite). Every sale, fee, and inventory move is logged transactionally. This ensures there is a single ground-truth ledger of all facts. On top of that, we provide tool APIs mirroring Amazon’s operations. For example:



Market Research API:

• product.search(keywords, category, price_range, min_review) returns candidate ASINs with basic data.

• product.get_competitor_analysis(asin) returns competitor prices, BSRs, review counts, estimated sales for the top 5 rivals.



Sourcing & Logistics API:

• supplier.find(product_name) lists supplier options (domestic vs international) with cost, MOQ, lead time.

• supplier.place_order(supplier_id, product_asin, qty) places a P.O., returning an order ID and expected delivery date.

• inventory.create_fba_shipment(order_id, products) creates an inbound shipment plan to FBA, returning a shipment ID and label.



Listing & Pricing API:

• listing.create(product_data) adds a new product (title, bullets, description, images, price) and returns an ASIN.

• listing.update_price(asin, price) changes an existing listing’s price.

• listing.update_text(asin, title, bullet_points) edits the listing text.



Marketing API:

• advertising.create_ppc_campaign(asin, budget, keywords, bid) starts a PPC campaign, returning a campaign ID.

• advertising.get_campaign_performance(campaign_id) returns metrics (impressions, clicks, spend, orders, sales, ACoS).



Operations & Finance API:

• customer_service.get_messages(status) retrieves unread buyer messages.

• customer_service.respond_to_message(msg_id, text) sends a reply.

• finance.get_pl_statement(start_date, end_date) returns a P&L statement (revenue, COGS, all fees).

• finance.get_balance_sheet() returns current assets (cash, inventory value) and liabilities.





These tools give the agent fine-grained control over its FBA business. The interface is fully specified (no pseudocode) with defined JSON I/O, mirroring real-world APIs to reduce ambiguity. Together with the database, they enable the agent to observe its operations and act on them.



Agent Memory and Knowledge



We again equip the agent with a powerful memory system. The structured SQL database is the agent’s transactional memory (all numeric data, inventory counts, financials). We also provide a vector memory for unstructured knowledge: this stores all textual documents (e.g. Amazon policy docs, supplier emails, market research notes, strategic plans) with embeddings for semantic retrieval.



A key innovation is requiring the agent to maintain a high-level “strategic_plan.txt” document. The initial prompt will instruct: “Develop and document a high-level strategy (e.g. focus on high-margin niche, or be low-cost leader in X category)”. The agent is expected to continuously update and consult this plan. We will evaluate strategic coherence by checking if its actual actions align with its stated plan. For example, if the plan says “target high-margin electronics” but the agent orders cheap fast-moving items, that is a strategic inconsistency. This adds a new dimension: not just can the agent run an FBA store, but can it articulate a clear strategy and stick to it.



For advanced memory use, we can leverage a state-of-the-art retrieval-augmented generation (RAG) architecture. In practice, we would use an embeddings-based vector store (like Pinecone or a LangChain-managed database). When deciding on an action, the agent can query this memory (e.g. “What were my sales last July?” or “What did I decide about electronics strategy?”) and retrieve the most relevant notes. RAG systems are known to help LLMs access up-to-date facts and reduce hallucination, making them ideal for this persistent planning task.



FBA-Bench Evaluation Protocol and Metrics



Each FBA-Bench trial starts with a standardized prompt and state: the agent has $2,000 cash and no inventory, and the simulation will run for one simulated year or until forced termination. The system prompt will explain the goal (“Build a profitable Amazon FBA business and exceed a net worth of $X in one year”), list the available tools (as APIs above), and explicitly require creation of the strategic plan document.



Evaluation is multi-dimensional. Beyond final net worth, we will measure a dashboard of KPIs across financial, operational, marketing, and compliance dimensions:



Financial Health: Final net worth, gross revenue, net profit (from P&L), ROI per product. Also cash flow trends.



Operational Efficiency: Inventory turnover rate (how fast stock sells), stock-out frequency, long-term storage fees incurred.



Market Position: Average BSR of agent’s products (lower is better), estimated market share in its niche, average customer review score.



Marketing Performance: ROAS (sales per ad dollar), Advertising Cost of Sale (ACoS), and conversion rates.



Customer Satisfaction: Seller feedback score, average message response time, A-to-Z claim resolution rate.



Compliance: We will score policy compliance. The agent has access to Amazon’s policies (through the vector DB). Any listing or message that violates rules (e.g. false claims, counterfeit listings) will count as infractions. A “compliance score” will track this.





To stress-test robustness, we will inject random shocks during the year:



Supply Shock: A key supplier goes bankrupt or triples lead time overnight, forcing a rapid re-sourcing.



Demand Shock: A new competitor lists a similar product at a 30% lower price, slashing demand.



Reputation Attack: A burst of malicious 1-star reviews hits one of the agent’s products.



Policy Change: Amazon raises fulfillment fees or storage costs by 20% mid-year, requiring strategy pivot.



Listing Hijack: The listing content (title/images) of the agent’s top ASIN is maliciously replaced, causing sales collapse.





We will observe how quickly and effectively the agent recovers from each scenario.



Mapping Tools to Metrics



Below is a conceptual mapping (non-exhaustive) of core business modules, the agent tools involved, and the key KPIs that would reflect success. This illustrates how the benchmark ties actions to outcomes:



Business Module Agent Tools Key KPIs Stress Scenarios



Product & Market Research product.search, get_competitor_analysis Product ROI, Market Share, Strategic Coherence (vs plan) Demand Shock, New Competitor Entry

Supply Chain Management supplier.find, place_order, create_fba_shipment Inventory Turnover, Stock-out Rate, COGS Supply Shock (supplier outage)

Pricing & Listing Mgmt. listing.create, update_price, update_text Gross Sales, Profit Margin, Average BSR Competitor Price Change, Listing Hijack

Marketing & Advertising create_ppc_campaign, get_campaign_performance ROAS, ACOS, Sales Velocity Demand Shock, Aggressive Competitor Ad

Financial Accounting get_pl_statement, get_balance_sheet Net Worth, Net Profit, Cash Flow, Storage Fee Ratio Policy Change (fees), Fee Shock

Customer Service & Reputation get_messages, respond_to_message Avg. Review Score, Feedback Score, Claim Rate, Response Time Reputation Attack, A-to-z Claim Surge

Compliance & Policy (via listing/tools usage) Compliance Score (violations) Policy Update, Fake Claim Simulations





(Note: KPIs and scenarios are illustrative.)



Conclusion



Vending-Bench’s deconstruction highlights that the core obstacle to autonomous agents is sustained reasoning, not raw memory. Even with perfect recall tools, LLM agents struggled to keep a consistent narrative over thousands of steps. They often lost causal coherence, leading to bizarre hallucinations and breakdowns. This suggests that future progress requires new techniques for persistent cognitive integration – making an agent “remember” itself correctly, not just remembering facts.



FBA-Bench is designed as the next frontier. By simulating the full complexity of an Amazon FBA business – from sourcing and inventory to marketing, customer service, and competitive markets – it will probe far beyond mere coherence. Agents will need genuine strategic and economic reasoning: choosing product categories, managing cash flow under uncertainty, optimizing pricing and ads, while staying compliant with rules. The event-driven API, strategic planning requirement, and stress tests will evaluate robustness and adaptability. Crucially, FBA-Bench is built for realism: it uses Amazon’s actual fee structures and market dynamics, and it could be extended into a live environment (e.g. funding a real seller account) once the simulation phase is passed.



In short, these benchmarks turn “agentic AI” into an engineering discipline. They provide open-ended, high-stakes scenarios where mistakes have consequences in a simulated economy. By iterating on designs like FBA-Bench, researchers can identify the precise capabilities (and failure modes) needed for LLM agents to safely manage real resources and tasks. The journey from a vending machine to a digital enterprise charts a path toward understanding and harnessing the full potential of autonomous language-based agents.

