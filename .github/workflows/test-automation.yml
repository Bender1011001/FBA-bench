name: FBA-Bench Test Automation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  # Python backend tests
  python-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, "3.10", "3.11"]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run linters
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
        black --check .
        isort --check-only .
    
    - name: Run security checks
      run: |
        bandit -r . -f json -o bandit-report.json || true
        safety check --json || true
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=. --cov-report=xml --cov-report=term-missing --junitxml=python-unit-results.xml
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v --junitxml=python-integration-results.xml
    
    - name: Run validation tests
      run: |
        pytest tests/validation/ -v --junitxml=python-validation-results.xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: python
        name: codecov-python
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: python-test-results-${{ matrix.python-version }}
        path: |
          python-unit-results.xml
          python-integration-results.xml
          python-validation-results.xml
          coverage.xml
          htmlcov/
          bandit-report.json

  # Frontend tests
  frontend-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install frontend dependencies
      working-directory: ./frontend
      run: |
        npm ci
    
    - name: Run linters
      working-directory: ./frontend
      run: |
        npm run lint
        npm run format:check
    
    - name: Run security audit
      working-directory: ./frontend
      run: |
        npm audit --audit-level moderate
    
    - name: Run unit tests
      working-directory: ./frontend
      run: |
        npm test -- --coverage --watchAll=false --ci
    
    - name: Run accessibility tests
      working-directory: ./frontend
      run: |
        npm run test:accessibility
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./frontend/coverage/lcov.info
        flags: frontend
        name: codecov-frontend
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: frontend-test-results
        path: |
          frontend/coverage/
          frontend/test-results/

  # Performance tests
  performance-tests:
    runs-on: ubuntu-latest
    needs: [python-tests, frontend-tests]
    if: github.event_name == 'schedule' || contains(github.event.pull_request.labels.*.name, 'performance')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run performance benchmarks
      run: |
        pytest tests/performance/ -v --benchmark-only --benchmark-json=benchmark-results.json
    
    - name: Compare with baseline
      run: |
        python scripts/compare_benchmarks.py benchmark-results.json
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: performance-test-results
        path: |
          benchmark-results.json
          benchmark-comparison-report.html

  # End-to-end tests
  e2e-tests:
    runs-on: ubuntu-latest
    needs: [python-tests, frontend-tests]
    if: github.event_name == 'schedule' || contains(github.event.pull_request.labels.*.name, 'e2e')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install dependencies
      working-directory: ./frontend
      run: |
        npm ci
    
    - name: Run E2E tests
      working-directory: ./frontend
      run: |
        npm run test:e2e
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: |
          cypress/videos/
          cypress/screenshots/
          cypress/results/

  # Load tests
  load-tests:
    runs-on: ubuntu-latest
    needs: [python-tests, frontend-tests]
    if: github.event_name == 'schedule' || contains(github.event.pull_request.labels.*.name, 'load')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install locust
    
    - name: Run load tests
      run: |
        locust -f tests/load/locustfile.py --headless --users 100 --spawn-rate 10 --run-time 1m --html load-test-report.html
    
    - name: Upload load test results
      uses: actions/upload-artifact@v3
      with:
        name: load-test-results
        path: |
          load-test-report.html

  # Test reporting
  test-reporting:
    runs-on: ubuntu-latest
    needs: [python-tests, frontend-tests, performance-tests, e2e-tests, load-tests]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v3
    
    - name: Generate test report
      run: |
        python scripts/generate_test_report.py
    
    - name: Upload test report
      uses: actions/upload-artifact@v3
      with:
        name: test-report
        path: |
          test-report.html
          test-summary.json
    
    - name: Comment PR with test results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const testSummary = JSON.parse(fs.readFileSync('test-summary.json', 'utf8'));
          
          let comment = `## Test Results Summary\n\n`;
          comment += `| Test Category | Status | Details |\n`;
          comment += `|---------------|--------|---------|\n`;
          
          for (const [category, results] of Object.entries(testSummary)) {
            const status = results.passed ? 'âœ… Pass' : 'âŒ Fail';
            comment += `| ${category} | ${status} | ${results.passed}/${results.total} tests passed |\n`;
          }
          
          comment += `\nFull test report available in the artifacts.\n`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  # Quality gates
  quality-gates:
    runs-on: ubuntu-latest
    needs: [python-tests, frontend-tests, performance-tests, e2e-tests, load-tests]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v3
    
    - name: Check quality gates
      run: |
        python scripts/check_quality_gates.py
    
    - name: Create quality badge
      if: success()
      run: |
        echo "ðŸŸ¢ All quality gates passed" > quality-status.txt
    
    - name: Create quality badge
      if: failure()
      run: |
        echo "ðŸ”´ Quality gates failed" > quality-status.txt
    
    - name: Upload quality status
      uses: actions/upload-artifact@v3
      with:
        name: quality-status
        path: quality-status.txt

  # Deployment (only on main branch after all tests pass)
  deployment:
    runs-on: ubuntu-latest
    needs: [quality-gates]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download quality status
      uses: actions/download-artifact@v3
      with:
        name: quality-status
    
    - name: Check quality status
      run: |
        if [ "$(cat quality-status.txt)" != "ðŸŸ¢ All quality gates passed" ]; then
          echo "Cannot deploy: quality gates not passed"
          exit 1
        fi
    
    - name: Deploy to production
      run: |
        echo "Deploying to production..."
        # Add deployment commands here