name: FBA-Bench Leaderboard Update

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch: # Allows manual trigger
    inputs:
      DUMMY_GPT35_RESULT_JSON:
        description: 'Dummy GPT-3.5 Result JSON'
        required: false
        default: '{"score": 36.5, "breakdown": {"finance": 0.5, "cost": -0.01}, "cost_usd": 0.012, "token_usage": 550, "evaluation_period": "2025-07-29T10:00:00Z", "tier": "T0"}'
      DUMMY_CLAUDE_RESULT_JSON:
        description: 'Dummy Claude 3.5 Sonnet Result JSON'
        required: false
        default: '{"score": 68.9, "breakdown": {"finance": 0.8, "cost": -0.04}, "cost_usd": 0.05, "token_usage": 2000, "evaluation_period": "2025-07-29T10:05:00Z", "tier": "T1"}'
      DUMMY_GREEDY_RESULT_JSON:
        description: 'Dummy GreedyScript Result JSON'
        required: false
        default: '{"score": 8.5, "breakdown": {"finance": 0.1, "cost": 0.0}, "cost_usd": 0.00, "token_usage": 0, "evaluation_period": "2025-07-29T10:10:00Z", "tier": "T0"}'

jobs:
  run_benchmarks_and_update_leaderboard:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9' # Or your preferred Python version

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Additional installs for OpenRouter API key if not in requirements (e.g. for testing)
          # pip install python-dotenv # if you use a .env file locally

      - name: Run Benchmarks and Update Leaderboard
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          DUMMY_GPT35_RESULT_JSON: ${{ github.event.inputs.DUMMY_GPT35_RESULT_JSON }}
          DUMMY_CLAUDE_RESULT_JSON: ${{ github.event.inputs.DUMMY_CLAUDE_RESULT_JSON }}
          DUMMY_GREEDY_RESULT_JSON: ${{ github.event.inputs.DUMMY_GREEDY_RESULT_JSON }}
        run: |
          echo "Simulating tests for bots and collecting results..."
          mkdir -p artifacts
          echo "{}" > artifacts/scores.json

          python -c "
          import json
          import os
          import asyncio
          from leaderboard.score_tracker import ScoreTracker
          from leaderboard.leaderboard_manager import LeaderboardManager
          from leaderboard.leaderboard_renderer import LeaderboardRenderer

          tracker = ScoreTracker()
          renderer = LeaderboardRenderer(template_path='leaderboard/templates')
          manager = LeaderboardManager(tracker, renderer)

          # Default results for when workflow_dispatch inputs are not provided
          default_gpt35 = {'score': 36.5, 'breakdown': {'finance': 0.5, 'cost': -0.01}, 'cost_usd': 0.012, 'token_usage': 550, 'evaluation_period': '2025-07-29T10:00:00Z', 'tier': 'T0'}
          default_claude = {'score': 68.9, 'breakdown': {'finance': 0.8, 'cost': -0.04}, 'cost_usd': 0.05, 'token_usage': 2000, 'evaluation_period': '2025-07-29T10:05:00Z', 'tier': 'T1'}
          default_greedy = {'score': 8.5, 'breakdown': {'finance': 0.1, 'cost': 0.0}, 'cost_usd': 0.00, 'token_usage': 0, 'evaluation_period': '2025-07-29T10:10:00Z', 'tier': 'T0'}

          # Get dummy results from environment variables or use defaults
          dummy_gpt35_result = default_gpt35
          dummy_claude_result = default_claude
          dummy_greedy_result = default_greedy

          try:
              if os.environ.get('DUMMY_GPT35_RESULT_JSON'):
                  dummy_gpt35_result = json.loads(os.environ['DUMMY_GPT35_RESULT_JSON'])
          except json.JSONDecodeError:
              print('Warning: Could not parse DUMMY_GPT35_RESULT_JSON, using default')

          try:
              if os.environ.get('DUMMY_CLAUDE_RESULT_JSON'):
                  dummy_claude_result = json.loads(os.environ['DUMMY_CLAUDE_RESULT_JSON'])
          except json.JSONDecodeError:
              print('Warning: Could not parse DUMMY_CLAUDE_RESULT_JSON, using default')

          try:
              if os.environ.get('DUMMY_GREEDY_RESULT_JSON'):
                  dummy_greedy_result = json.loads(os.environ['DUMMY_GREEDY_RESULT_JSON'])
          except json.JSONDecodeError:
              print('Warning: Could not parse DUMMY_GREEDY_RESULT_JSON, using default')

          print('Adding dummy GPT-3.5 result...')
          manager.score_tracker.add_run_result('GPT-3.5', dummy_gpt35_result.get('tier', 'T0'), dummy_gpt35_result.get('score', 0.0), dummy_gpt35_result)
          print('Adding dummy Claude 3.5 Sonnet result...')
          manager.score_tracker.add_run_result('Claude 3.5 Sonnet', dummy_claude_result.get('tier', 'T1'), dummy_claude_result.get('score', 0.0), dummy_claude_result)
          print('Adding dummy GreedyScript result...')
          manager.score_tracker.add_run_result('GreedyScript', dummy_greedy_result.get('tier', 'T0'), dummy_greedy_result.get('score', 0.0), dummy_greedy_result)

          print('Generating leaderboard artifacts...')
          async def generate():
              await manager.generate_leaderboard_artifacts()
          asyncio.run(generate())

          latest_leaderboard_data = manager.leaderboard_renderer.get_latest_leaderboard_data()
          if latest_leaderboard_data:
              badge_markdown = manager.leaderboard_renderer.generate_badge_markdown(latest_leaderboard_data)
              with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
                  print(f'badge_markdown={badge_markdown}', file=fh)
          "

      - name: Upload Leaderboard Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: fba-bench-leaderboard
          path: artifacts/

      - name: Update README.md with Leaderboard Badge
        if: github.ref == 'refs/heads/main'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          README_FILE="README.md"
          BADGE_START="<!-- LEADERBOARD_BADGE_START -->"
          BADGE_END="<!-- LEADERBOARD_BADGE_END -->"
          NEW_BADGE="${{ steps.run_benchmarks_and_update_leaderboard.outputs.badge_markdown }}"

          if [ ! -f "$README_FILE" ]; then
            touch "$README_FILE"
          fi

          if grep -q "$BADGE_START" "$README_FILE" && grep -q "$BADGE_END" "$README_FILE"; then
              awk -v start="$BADGE_START" -v end="$BADGE_END" -v new_badge="$NEW_BADGE" '
                  BEGIN { in_badge_section=0 }
                  {
                      if ($0 ~ start) { print; print new_badge; in_badge_section=1; }
                      else if ($0 ~ end) { print; in_badge_section=0; }
                      else if (in_badge_section==0) { print; }
                  }
              ' "$README_FILE" > temp_readme.md && mv temp_readme.md "$README_FILE"
              echo "Updated existing leaderboard badge in $README_FILE"
          else
              echo -e "\n$BADGE_START\n$NEW_BADGE\n$BADGE_END" >> "$README_FILE"
              echo "Appended new leaderboard badge to $README_FILE"
          fi

          git add "$README_FILE"
          git commit -m "Update leaderboard badge [skip ci]" || echo "No changes to commit"
          git push

  regression_detection: # Separated into a new job for clarity and dependency management
    runs-on: ubuntu-latest
    needs: run_benchmarks_and_update_leaderboard # Ensures this runs after artifacts are available
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Download Leaderboard Artifacts
        uses: actions/download-artifact@v3
        with:
          name: fba-bench-leaderboard
          path: artifacts/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies for CI integration
        run: |
          python -m pip install --upgrade pip
          pip install requests # For GitHub API calls

      - name: Run Regression Detection
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python -c "
          import json
          import os
          from leaderboard.ci_integration import CIIntegration

          ci_integrator = CIIntegration(artifacts_dir='artifacts')
          new_data = ci_integrator.get_latest_leaderboard_data()

          previous_data = None
          # In a real setup, fetch the previous leaderboard artifact from a successful main branch run.
          # This example simulates it.
          previous_artifacts_path = 'artifacts/previous_leaderboard.json'
          if os.path.exists(previous_artifacts_path):
              try:
                  with open(previous_artifacts_path, 'r') as f:
                      previous_data = json.load(f)
              except json.JSONDecodeError:
                  print(f'Warning: Could not decode JSON from {previous_artifacts_path}')
                  previous_data = None
          else:
              print('No previous leaderboard.json found for regression comparison. Assuming no regression for first run.')

          if new_data:
              regressions = ci_integrator.detect_regression(new_data, previous_data)
              if regressions:
                  print('::error title=Leaderboard Regression Detected::' + '\\n'.join(regressions))
                  exit(1) # Fail the CI job
              else:
                  print('No significant regressions detected.')
          else:
              print('No new leaderboard data to check for regressions (leaderboard.json not found).')
          "