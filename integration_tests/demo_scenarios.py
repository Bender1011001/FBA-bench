"""
Demo Scenarios for FBA-Bench Tier-1 Capabilities

This module provides comprehensive demonstration scenarios that showcase
FBA-Bench's tier-1 capabilities in action. These demos serve as both
validation tests and examples for researchers and developers.

Demo Scenarios:
1. T0 Baseline Demo - Simple scenario with GPT-4o performing basic tasks
2. T3 Stress Test - Complex multi-shock scenario with memory-limited agent  
3. Framework Comparison - Same scenario run with DIY, CrewAI, and LangChain
4. Adversarial Resistance - Agent facing multiple exploit attempts
5. Memory Ablation - Compare agent with 7-day vs unlimited memory

Each demo includes:
- Scenario setup and configuration
- Agent initialization and execution
- Real-time monitoring and metrics
- Results analysis and visualization
- Performance benchmarking
"""

import asyncio
import logging
import time
import json
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict

# Core imports
from integration_tests import IntegrationTestSuite, IntegrationTestConfig, logger
from simulation_orchestrator import SimulationOrchestrator, SimulationConfig
from event_bus import get_event_bus, EventBus
from metrics.metric_suite import MetricSuite, STANDARD_WEIGHTS
from constraints.budget_enforcer import BudgetEnforcer
from reproducibility.sim_seed import SimSeed

# Agent framework imports
from agent_runners.runner_factory import RunnerFactory
from agent_runners.configs.framework_configs import FrameworkConfig
from baseline_bots.bot_factory import BotFactory

# Memory experiment imports
from memory_experiments.experiment_runner import ExperimentRunner
from memory_experiments.memory_config import MemoryConfig
from memory_experiments.memory_modes import MemoryMode

# Adversarial testing imports
from redteam.gauntlet_runner import GauntletRunner
from redteam.adversarial_event_injector import AdversarialEventInjector
from redteam.resistance_scorer import AdversaryResistanceScorer as ResistanceScorer

# Services imports
from services.world_store import WorldStore
from services.sales_service import SalesService
from services.trust_score_service import TrustScoreService
from financial_audit import FinancialAuditService

@dataclass
class DemoResults:
    """Results from a demo scenario run."""
    scenario_name: str
    start_time: datetime
    end_time: datetime
    duration_seconds: float
    final_score: float
    score_breakdown: Dict[str, float]
    event_count: int
    tick_count: int
    agent_info: Dict[str, Any]
    performance_metrics: Dict[str, Any]
    success: bool
    notes: str = ""

class DemoScenarios:
    """Demo scenarios showcasing FBA-Bench tier-1 capabilities."""
    
    def __init__(self, config: IntegrationTestConfig = None):
        self.config = config or IntegrationTestConfig(verbose_logging=True)
        self.demo_results = []
        
    async def run_t0_baseline_demo(self) -> DemoResults:
        """
        T0 Baseline Demo: Simple scenario with baseline bot performing basic tasks.
        
        Showcases:
        - Basic FBA operations in stable market
        - Baseline agent performance measurement
        - Fundamental scoring system
        - Event-driven architecture
        """
        logger.info("ðŸŽ¯ Running T0 Baseline Demo...")
        
        start_time = datetime.now()
        
        # Configure T0 baseline scenario
        demo_config = SimulationConfig(
            seed=42,
            max_ticks=100,
            tick_interval_seconds=0.01,
            time_acceleration=50.0
        )
        
        # Create T0 environment
        env = await self._create_demo_environment("T0", 42)
        
        # Initialize baseline bot
        try:
            agent = BotFactory.create_bot("gpt_4o_mini_bot")  # Use available bot
            if not agent:
                agent = BotFactory.create_bot("greedy_script_bot")  # Fallback
            agent_type = "gpt_4o_mini_bot" if agent else "fallback"
        except Exception as e:
            logger.warning(f"Failed to create advanced bot: {e}")
            agent = BotFactory.create_bot("greedy_script_bot")
            agent_type = "greedy_script_bot"
        
        # Initialize T0 constraints (minimal)
        budget_enforcer = BudgetEnforcer.from_tier_config("T0", env["event_bus"])
        
        # Initialize metrics
        financial_audit = FinancialAuditService()
        metric_suite = MetricSuite(
            tier="T0",
            financial_audit_service=financial_audit,
            sales_service=env["services"]["sales"],
            trust_score_service=env["services"]["trust"]
        )
        metric_suite.subscribe_to_events(env["event_bus"])
        
        # Run T0 simulation
        orchestrator = env["orchestrator"]
        event_bus = env["event_bus"]
        
        logger.info("Starting T0 baseline simulation...")
        
        event_bus.start_recording()
        await orchestrator.start(event_bus)
        
        # Generate some basic events for demo
        demo_events = [
            {"type": "SaleOccurred", "data": {"amount": 100, "product": "demo_product"}},
            {"type": "SetPriceCommand", "data": {"product": "demo_product", "price": 25}},
            {"type": "AgentDecisionEvent", "data": {"decision": "maintain_inventory", "confidence": 0.8}}
        ]
        
        for event in demo_events:
            event_bus.publish(event["type"], event["data"])
            await asyncio.sleep(0.1)
        
        # Run for demo duration
        demo_duration = 3  # 3 seconds
        await asyncio.sleep(demo_duration)
        
        await orchestrator.stop()
        
        # Collect results
        events = event_bus.get_recorded_events()
        event_bus.stop_recording()
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # Calculate final metrics
        if events:
            final_scores = metric_suite.calculate_final_score(events)
            final_score = final_scores.score
            score_breakdown = final_scores.breakdown
        else:
            final_score = 0
            score_breakdown = {}
        
        # Performance metrics
        performance_metrics = {
            "ticks_per_second": orchestrator.current_tick / duration if duration > 0 else 0,
            "events_per_second": len(events) / duration if duration > 0 else 0,
            "token_usage": budget_enforcer.total_simulation_tokens_used
        }
        
        demo_result = DemoResults(
            scenario_name="T0_Baseline_Demo",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            final_score=final_score,
            score_breakdown=score_breakdown,
            event_count=len(events),
            tick_count=orchestrator.current_tick,
            agent_info={"type": agent_type, "framework": "baseline"},
            performance_metrics=performance_metrics,
            success=final_score > 0 and len(events) > 0,
            notes=f"T0 baseline demo with {agent_type} agent. Basic FBA operations in stable market."
        )
        
        logger.info(f"T0 Baseline Demo completed:")
        logger.info(f"  Duration: {duration:.2f}s")
        logger.info(f"  Final Score: {final_score:.2f}")
        logger.info(f"  Events: {len(events)}")
        logger.info(f"  Performance: {performance_metrics['ticks_per_second']:.1f} ticks/sec")
        
        self.demo_results.append(demo_result)
        return demo_result
    
    async def run_t3_stress_test_demo(self) -> DemoResults:
        """
        T3 Stress Test: Complex multi-shock scenario with memory-limited agent.
        
        Showcases:
        - Advanced tier constraints (T3)
        - Multiple shock event handling
        - Memory constraint enforcement
        - Adversarial resistance testing
        - Cognitive resilience measurement
        """
        logger.info("ðŸŽ¯ Running T3 Stress Test Demo...")
        
        start_time = datetime.now()
        
        # Configure T3 stress scenario
        demo_config = SimulationConfig(
            seed=1337,
            max_ticks=200,
            tick_interval_seconds=0.01,
            time_acceleration=30.0
        )
        
        # Create T3 environment
        env = await self._create_demo_environment("T3", 1337)
        
        # Initialize advanced agent with constraints
        try:
            agent = BotFactory.create_bot("claude_sonnet_bot")
            if not agent:
                agent = BotFactory.create_bot("gpt_4o_mini_bot")
            if not agent:
                agent = BotFactory.create_bot("greedy_script_bot")
            agent_type = "claude_sonnet_bot" if agent else "fallback"
        except Exception as e:
            logger.warning(f"Failed to create advanced bot: {e}")
            agent = BotFactory.create_bot("greedy_script_bot")
            agent_type = "greedy_script_bot"
        
        # Initialize T3 constraints (maximum)
        budget_enforcer = BudgetEnforcer.from_tier_config("T3", env["event_bus"])
        
        # Initialize memory constraints (ablated for stress test)
        try:
            memory_config = MemoryConfig(
                mode=MemoryMode.ABLATED,
                max_memory_days=7,
                memory_systems=["scratchpad"],
                memory_size_limit="100MB"
            )
            memory_constrained = True
        except Exception:
            memory_constrained = False
        
        # Initialize metrics for T3
        financial_audit = FinancialAuditService()
        metric_suite = MetricSuite(
            tier="T3",
            financial_audit_service=financial_audit,
            sales_service=env["services"]["sales"],
            trust_score_service=env["services"]["trust"]
        )
        metric_suite.subscribe_to_events(env["event_bus"])
        
        # Initialize adversarial testing
        try:
            adversarial_injector = AdversarialEventInjector()
            resistance_scorer = ResistanceScorer()
            adversarial_available = True
        except Exception:
            adversarial_available = False
        
        # Run T3 stress simulation
        orchestrator = env["orchestrator"]
        event_bus = env["event_bus"]
        
        logger.info("Starting T3 stress test simulation...")
        
        event_bus.start_recording()
        await orchestrator.start(event_bus)
        
        # Inject stress events throughout simulation
        stress_events = [
            # Initial stable operation
            {"delay": 0.5, "type": "SaleOccurred", "data": {"amount": 200, "product": "stress_product"}},
            
            # Shock 1: Fee hike (financial stress)
            {"delay": 1.0, "type": "ShockInjectionEvent", "data": {"shock_type": "fee_hike", "intensity": 0.3}},
            
            # Shock 2: Supply delay (operational stress)
            {"delay": 1.5, "type": "ShockInjectionEvent", "data": {"shock_type": "supply_delay", "intensity": 0.4}},
            
            # Shock 3: Review bombing (adversarial stress)
            {"delay": 2.0, "type": "AdversarialEvent", "data": {"exploit_type": "review_bomb", "severity": 0.6}},
            
            # Shock 4: Competitor price war (market stress)
            {"delay": 2.5, "type": "ShockInjectionEvent", "data": {"shock_type": "price_war", "intensity": 0.5}},
            
            # Recovery period with continued pressure
            {"delay": 3.0, "type": "AgentDecisionEvent", "data": {"decision": "crisis_response", "stress_level": 0.8}}
        ]
        
        # Schedule stress events
        async def inject_stress_events():
            for event in stress_events:
                await asyncio.sleep(event["delay"])
                event_bus.publish(event["type"], event["data"])
                logger.info(f"  Injected stress event: {event['type']}")
        
        # Run stress injection concurrently
        stress_task = asyncio.create_task(inject_stress_events())
        
        # Run for stress test duration
        demo_duration = 4  # 4 seconds of intense stress
        await asyncio.sleep(demo_duration)
        
        # Ensure stress events complete
        try:
            await asyncio.wait_for(stress_task, timeout=1.0)
        except asyncio.TimeoutError:
            pass
        
        await orchestrator.stop()
        
        # Collect results
        events = event_bus.get_recorded_events()
        event_bus.stop_recording()
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # Calculate final metrics
        if events:
            final_scores = metric_suite.calculate_final_score(events)
            final_score = final_scores.score
            score_breakdown = final_scores.breakdown
        else:
            final_score = 0
            score_breakdown = {}
        
        # Calculate adversarial resistance if available
        ars_score = 0
        if adversarial_available:
            try:
                adversarial_events = [e for e in events if e.get("type") == "AdversarialEvent"]
                if adversarial_events:
                    ars_score, _ = resistance_scorer.calculate_ars(adversarial_events)
            except Exception:
                pass
        
        # Performance metrics
        performance_metrics = {
            "ticks_per_second": orchestrator.current_tick / duration if duration > 0 else 0,
            "events_per_second": len(events) / duration if duration > 0 else 0,
            "token_usage": budget_enforcer.total_simulation_tokens_used,
            "stress_events_handled": len([e for e in events if "Shock" in e.get("type", "")]),
            "adversarial_resistance_score": ars_score,
            "memory_constrained": memory_constrained
        }
        
        # Success criteria for T3 stress test
        success = (
            final_score > 40 and  # Maintain reasonable performance under stress
            len(events) > 20 and  # Generate sufficient activity
            performance_metrics["stress_events_handled"] > 0  # Handle stress events
        )
        
        demo_result = DemoResults(
            scenario_name="T3_Stress_Test_Demo",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            final_score=final_score,
            score_breakdown=score_breakdown,
            event_count=len(events),
            tick_count=orchestrator.current_tick,
            agent_info={"type": agent_type, "framework": "baseline", "memory_mode": "ablated"},
            performance_metrics=performance_metrics,
            success=success,
            notes=f"T3 stress test with {agent_type} agent. Multiple shock events with memory constraints."
        )
        
        logger.info(f"T3 Stress Test Demo completed:")
        logger.info(f"  Duration: {duration:.2f}s")
        logger.info(f"  Final Score: {final_score:.2f}")
        logger.info(f"  Stress Events: {performance_metrics['stress_events_handled']}")
        logger.info(f"  ARS Score: {ars_score:.2f}")
        logger.info(f"  Success: {success}")
        
        self.demo_results.append(demo_result)
        return demo_result
    
    async def run_framework_comparison_demo(self) -> List[DemoResults]:
        """
        Framework Comparison: Same scenario run with different frameworks.
        
        Showcases:
        - Multi-framework support (DIY, CrewAI, LangChain)
        - Framework abstraction layer
        - Performance comparison across frameworks
        - Consistent evaluation methodology
        """
        logger.info("ðŸŽ¯ Running Framework Comparison Demo...")
        
        # Test scenario configuration
        comparison_seed = 2023
        demo_duration = 2
        
        frameworks_to_test = [
            {"name": "DIY", "type": "diy"},
            {"name": "GreedyScript", "type": "greedy_script"},  # Available fallback
        ]
        
        # Try to add advanced frameworks if available
        try:
            frameworks_to_test.extend([
                {"name": "CrewAI", "type": "crewai"},
                {"name": "LangChain", "type": "langchain"}
            ])
        except Exception:
            logger.info("Advanced frameworks not available for demo")
        
        comparison_results = []
        
        for framework_info in frameworks_to_test[:2]:  # Limit to available frameworks
            framework_name = framework_info["name"]
            framework_type = framework_info["type"]
            
            logger.info(f"Testing framework: {framework_name}...")
            
            start_time = datetime.now()
            
            # Create environment for framework
            env = await self._create_demo_environment("T1", comparison_seed)
            
            # Initialize framework-specific agent
            try:
                if framework_type == "greedy_script":
                    agent = BotFactory.create_bot("greedy_script_bot")
                elif framework_type == "diy":
                    # Mock DIY framework
                    agent = BotFactory.create_bot("greedy_script_bot")  # Use as placeholder
                else:
                    # Advanced frameworks
                    config = FrameworkConfig(framework_type=framework_type)
                    agent = RunnerFactory.create_runner(framework_type, config)
                
                agent_created = agent is not None
            except Exception as e:
                logger.warning(f"Framework {framework_name} not available: {e}")
                agent_created = False
                continue
            
            # Initialize T1 constraints
            budget_enforcer = BudgetEnforcer.from_tier_config("T1", env["event_bus"])
            
            # Initialize metrics
            financial_audit = FinancialAuditService()
            metric_suite = MetricSuite(
                tier="T1",
                financial_audit_service=financial_audit,
                sales_service=env["services"]["sales"],
                trust_score_service=env["services"]["trust"]
            )
            metric_suite.subscribe_to_events(env["event_bus"])
            
            # Run framework simulation
            orchestrator = env["orchestrator"]
            event_bus = env["event_bus"]
            
            event_bus.start_recording()
            await orchestrator.start(event_bus)
            
            # Generate identical test events for all frameworks
            test_events = [
                {"type": "SaleOccurred", "data": {"amount": 150, "framework": framework_name}},
                {"type": "SetPriceCommand", "data": {"product": "comparison_product", "price": 30}},
                {"type": "AgentDecisionEvent", "data": {"framework": framework_name, "decision": "optimize"}},
                {"type": "MarketEvent", "data": {"type": "demand_spike", "intensity": 0.3}}
            ]
            
            for event in test_events:
                event_bus.publish(event["type"], event["data"])
                await asyncio.sleep(0.1)
            
            # Run for comparison duration
            await asyncio.sleep(demo_duration)
            
            await orchestrator.stop()
            
            # Collect results
            events = event_bus.get_recorded_events()
            event_bus.stop_recording()
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # Calculate metrics
            if events:
                final_scores = metric_suite.calculate_final_score(events)
                final_score = final_scores.score
                score_breakdown = final_scores.breakdown
            else:
                final_score = 0
                score_breakdown = {}
            
            # Performance metrics
            performance_metrics = {
                "ticks_per_second": orchestrator.current_tick / duration if duration > 0 else 0,
                "events_per_second": len(events) / duration if duration > 0 else 0,
                "token_usage": budget_enforcer.total_simulation_tokens_used,
                "framework_overhead": duration / demo_duration if demo_duration > 0 else 1.0
            }
            
            demo_result = DemoResults(
                scenario_name=f"Framework_Comparison_{framework_name}",
                start_time=start_time,
                end_time=end_time,
                duration_seconds=duration,
                final_score=final_score,
                score_breakdown=score_breakdown,
                event_count=len(events),
                tick_count=orchestrator.current_tick,
                agent_info={"type": framework_name, "framework": framework_type},
                performance_metrics=performance_metrics,
                success=final_score > 0 and len(events) > 0,
                notes=f"Framework comparison demo with {framework_name} framework."
            )
            
            comparison_results.append(demo_result)
            
            logger.info(f"  {framework_name}: Score={final_score:.2f}, Events={len(events)}")
        
        # Log comparison summary
        if len(comparison_results) > 1:
            logger.info("Framework Comparison Summary:")
            for result in comparison_results:
                framework = result.agent_info["type"]
                logger.info(f"  {framework}: {result.final_score:.2f} points, {result.performance_metrics['ticks_per_second']:.1f} ticks/sec")
        
        self.demo_results.extend(comparison_results)
        return comparison_results
    
    async def run_memory_ablation_demo(self) -> List[DemoResults]:
        """
        Memory Ablation: Compare agent with 7-day vs unlimited memory.
        
        Showcases:
        - Memory experiment framework
        - Memory constraint enforcement
        - Performance impact of memory limitations
        - Memory-dependent decision making
        """
        logger.info("ðŸŽ¯ Running Memory Ablation Demo...")
        
        memory_seed = 8888
        demo_duration = 3
        
        memory_conditions = [
            {
                "name": "Ablated_Memory",
                "mode": MemoryMode.ABLATED,
                "max_days": 7,
                "systems": ["scratchpad"],
                "size_limit": "100MB"
            },
            {
                "name": "Saturated_Memory", 
                "mode": MemoryMode.SATURATED,
                "max_days": None,
                "systems": ["vector_db", "scratchpad", "full_rag"],
                "size_limit": "2GB"
            }
        ]
        
        memory_results = []
        
        for condition in memory_conditions:
            condition_name = condition["name"]
            
            logger.info(f"Testing memory condition: {condition_name}...")
            
            start_time = datetime.now()
            
            # Create environment
            env = await self._create_demo_environment("T2", memory_seed)
            
            # Initialize memory configuration
            try:
                memory_config = MemoryConfig(
                    mode=condition["mode"],
                    max_memory_days=condition["max_days"],
                    memory_systems=condition["systems"],
                    memory_size_limit=condition["size_limit"]
                )
                memory_available = True
            except Exception:
                logger.warning(f"Memory framework not available for {condition_name}")
                memory_available = False
                continue
            
            # Initialize agent
            try:
                agent = BotFactory.create_bot("gpt_4o_mini_bot")
                if not agent:
                    agent = BotFactory.create_bot("greedy_script_bot")
                agent_type = "gpt_4o_mini_bot" if agent else "greedy_script_bot"
            except Exception:
                agent = BotFactory.create_bot("greedy_script_bot")
                agent_type = "greedy_script_bot"
            
            # Initialize T2 constraints
            budget_enforcer = BudgetEnforcer.from_tier_config("T2", env["event_bus"])
            
            # Initialize metrics
            financial_audit = FinancialAuditService()
            metric_suite = MetricSuite(
                tier="T2",
                financial_audit_service=financial_audit,
                sales_service=env["services"]["sales"],
                trust_score_service=env["services"]["trust"]
            )
            metric_suite.subscribe_to_events(env["event_bus"])
            
            # Run memory experiment
            orchestrator = env["orchestrator"]
            event_bus = env["event_bus"]
            
            event_bus.start_recording()
            await orchestrator.start(event_bus)
            
            # Generate memory-intensive events
            memory_events = [
                {"type": "SaleOccurred", "data": {"amount": 100, "memory_test": True}},
                {"type": "AgentDecisionEvent", "data": {"decision": "remember_pattern", "complexity": 0.8}},
                {"type": "HistoricalDataEvent", "data": {"lookback_days": 14, "pattern": "seasonal"}},
                {"type": "AgentDecisionEvent", "data": {"decision": "recall_strategy", "memory_dependent": True}},
                {"type": "LongTermPlanEvent", "data": {"horizon_days": 30, "requires_history": True}}
            ]
            
            for event in memory_events:
                event_bus.publish(event["type"], event["data"])
                await asyncio.sleep(0.2)  # Slower for memory processing
            
            # Run for memory test duration
            await asyncio.sleep(demo_duration)
            
            await orchestrator.stop()
            
            # Collect results
            events = event_bus.get_recorded_events()
            event_bus.stop_recording()
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # Calculate metrics
            if events:
                final_scores = metric_suite.calculate_final_score(events)
                final_score = final_scores.score
                score_breakdown = final_scores.breakdown
            else:
                final_score = 0
                score_breakdown = {}
            
            # Memory-specific performance metrics
            performance_metrics = {
                "ticks_per_second": orchestrator.current_tick / duration if duration > 0 else 0,
                "events_per_second": len(events) / duration if duration > 0 else 0,
                "token_usage": budget_enforcer.total_simulation_tokens_used,
                "memory_mode": condition["mode"].value if hasattr(condition["mode"], 'value') else str(condition["mode"]),
                "memory_systems": len(condition["systems"]),
                "memory_limit": condition["size_limit"]
            }
            
            demo_result = DemoResults(
                scenario_name=f"Memory_Ablation_{condition_name}",
                start_time=start_time,
                end_time=end_time,
                duration_seconds=duration,
                final_score=final_score,
                score_breakdown=score_breakdown,
                event_count=len(events),
                tick_count=orchestrator.current_tick,
                agent_info={"type": agent_type, "memory_condition": condition_name},
                performance_metrics=performance_metrics,
                success=final_score > 0 and len(events) > 0,
                notes=f"Memory ablation demo with {condition_name} configuration."
            )
            
            memory_results.append(demo_result)
            
            logger.info(f"  {condition_name}: Score={final_score:.2f}, Events={len(events)}")
        
        # Log memory comparison
        if len(memory_results) == 2:
            ablated = memory_results[0]
            saturated = memory_results[1]
            
            score_difference = saturated.final_score - ablated.final_score
            logger.info(f"Memory Impact Analysis:")
            logger.info(f"  Ablated Memory: {ablated.final_score:.2f}")
            logger.info(f"  Saturated Memory: {saturated.final_score:.2f}")
            logger.info(f"  Performance Delta: {score_difference:.2f}")
        
        self.demo_results.extend(memory_results)
        return memory_results
    
    async def _create_demo_environment(self, tier: str, seed: int) -> Dict[str, Any]:
        """Create a demo environment with all necessary components."""
        
        # Initialize core components
        sim_config = SimulationConfig(
            seed=seed,
            max_ticks=200,
            tick_interval_seconds=0.01,
            time_acceleration=50.0
        )
        
        orchestrator = SimulationOrchestrator(sim_config)
        event_bus = get_event_bus()
        
        # Initialize services
        world_store = WorldStore()
        sales_service = SalesService(world_store)
        trust_service = TrustScoreService()
        
        return {
            "orchestrator": orchestrator,
            "event_bus": event_bus,
            "world_store": world_store,
            "services": {
                "sales": sales_service,
                "trust": trust_service
            }
        }
    
    def generate_demo_report(self) -> Dict[str, Any]:
        """Generate comprehensive demo report."""
        
        if not self.demo_results:
            return {"error": "No demo results available"}
        
        # Calculate aggregate statistics
        total_demos = len(self.demo_results)
        successful_demos = sum(1 for result in self.demo_results if result.success)
        success_rate = successful_demos / total_demos if total_demos > 0 else 0
        
        avg_score = sum(result.final_score for result in self.demo_results) / total_demos
        avg_duration = sum(result.duration_seconds for result in self.demo_results) / total_demos
        total_events = sum(result.event_count for result in self.demo_results)
        
        # Performance summary
        performance_summary = {
            "average_ticks_per_second": sum(
                result.performance_metrics.get("ticks_per_second", 0) 
                for result in self.demo_results
            ) / total_demos,
            "average_events_per_second": sum(
                result.performance_metrics.get("events_per_second", 0)
                for result in self.demo_results  
            ) / total_demos,
            "total_token_usage": sum(
                result.performance_metrics.get("token_usage", 0)
                for result in self.demo_results
            )
        }
        
        # Demo-specific insights
        demo_insights = []
        
        for result in self.demo_results:
            insight = {
                "scenario": result.scenario_name,
                "score": result.final_score,
                "success": result.success,
                "key_metrics": {
                    "duration": result.duration_seconds,
                    "events": result.event_count,
                    "performance": result.performance_metrics.get("ticks_per_second", 0)
                },
                "notes": result.notes
            }
            demo_insights.append(insight)
        
        report = {
            "demo_summary": {
                "total_demos": total_demos,
                "successful_demos": successful_demos,
                "success_rate": success_rate,
                "average_score": avg_score,
                "average_duration_seconds": avg_duration,
                "total_events_generated": total_events
            },
            "performance_summary": performance_summary,
            "demo_insights": demo_insights,
            "tier1_capabilities_demonstrated": {
                "multi_dimensional_scoring": any("score_breakdown" in asdict(r) for r in self.demo_results),
                "curriculum_progression": any("T0" in r.scenario_name or "T3" in r.scenario_name for r in self.demo_results),
                "framework_compatibility": any("Framework_Comparison" in r.scenario_name for r in self.demo_results),
                "memory_experiments": any("Memory_Ablation" in r.scenario_name for r in self.demo_results),
                "stress_testing": any("Stress_Test" in r.scenario_name for r in self.demo_results)
            },
            "recommendations": self._generate_recommendations()
        }
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on demo results."""
        
        recommendations = []
        
        if not self.demo_results:
            return ["No demo results available for analysis"]
        
        # Performance recommendations
        avg_score = sum(result.final_score for result in self.demo_results) / len(self.demo_results)
        
        if avg_score < 50:
            recommendations.append("Consider optimizing agent configurations for better performance")
        elif avg_score > 80:
            recommendations.append("Excellent performance across demos - system ready for production")
        
        # Success rate recommendations
        success_rate = sum(1 for result in self.demo_results if result.success) / len(self.demo_results)
        
        if success_rate < 0.8:
            recommendations.append("Investigate failures in unsuccessful demo scenarios")
        
        # Framework-specific recommendations
        framework_demos = [r for r in self.demo_results if "Framework_Comparison" in r.scenario_name]
        if framework_demos:
            recommendations.append("Framework comparison completed - analyze performance differences")
        
        # Memory-specific recommendations
        memory_demos = [r for r in self.demo_results if "Memory_Ablation" in r.scenario_name]
        if len(memory_demos) >= 2:
            recommendations.append("Memory ablation study completed - review memory impact on performance")
        
        return recommendations

# Demo runner for easy execution
async def run_all_demos() -> Dict[str, Any]:
    """Run all demo scenarios and generate comprehensive report."""
    
    logger.info("ðŸš€ Starting FBA-Bench Tier-1 Capability Demos...")
    
    demo_suite = DemoScenarios()
    
    try:
        # Run all demo scenarios
        await demo_suite.run_t0_baseline_demo()
        await demo_suite.run_t3_stress_test_demo()
        await demo_suite.run_framework_comparison_demo()
        await demo_suite.run_memory_ablation_demo()
        
        # Generate comprehensive report
        report = demo_suite.generate_demo_report()
        
        logger.info("ðŸŽ‰ All demo scenarios completed successfully!")
        
        return report
        
    except Exception as e:
        logger.error(f"Demo execution failed: {e}")
        raise

if __name__ == "__main__":
    # Run demos when script is executed directly
    asyncio.run(run_all_demos())