import os
import hashlib
import json
import pandas as pd
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import datetime
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)

@dataclass
class LLMInteractionLog:
    """Log entry for LLM interactions."""
    timestamp: str
    prompt_hash: str
    model: str
    temperature: float
    cache_hit: bool
    response_hash: str
    deterministic_mode: bool
    validation_passed: bool
    response_time_ms: float
    
@dataclass
class SnapshotMetadata:
    """Enhanced metadata for event snapshots."""
    simulation_mode: str
    master_seed: Optional[int]
    llm_cache_status: Dict[str, Any]
    determinism_validation: Dict[str, Any]
    snapshot_version: str = "2.0"
    reproducibility_features_enabled: Dict[str, bool] = None

# Ensure the parent directory for artifacts exists
ARTIFACTS_DIR = Path("artifacts")
ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

class EventSnapshot:
    """
    Enhanced event snapshot management with LLM interaction logging and
    reproducibility metadata integration.
    
    Manages the dumping and comparison of simulation event streams with
    support for deterministic LLM tracking and golden master validation.
    """
    
    # Class-level storage for LLM interactions
    _llm_interactions: List[LLMInteractionLog] = []
    _snapshot_metadata: Optional[SnapshotMetadata] = None

    @staticmethod
    def _create_event_df(events: List[Dict[str, Any]]) -> pd.DataFrame:
        """Converts a list of event dictionaries into a Pandas DataFrame."""
        if not events:
            return pd.DataFrame() # Return empty DataFrame if no events
        
        # Ensure all events have a 'timestamp' for sorting and consistency
        for event in events:
            if 'timestamp' not in event:
                event['timestamp'] = datetime.datetime.now(datetime.timezone.utc).isoformat()
        
        df = pd.DataFrame(events)
        # Ensure consistent column order for reproducibility, if possible.
        # This might be tricky with varying event types, but common columns first.
        common_cols = ['timestamp', 'event_type', 'data'] # Assuming these are common
        other_cols = [col for col in df.columns if col not in common_cols]
        df = df[common_cols + sorted(other_cols)]
        return df

    @staticmethod
    def dump_events(events: List[Dict[str, Any]], git_sha: str, run_id: str) -> Path:
        """
        Dumps the given event stream to a parquet file in the artifacts directory.
        Filename format: artifacts/<git_sha>_<run_id>.parquet
        """
        if not events:
            print("Warning: No events to dump. Skipping snapshot creation.")
            return None

        file_name = f"{git_sha}_{run_id}.parquet"
        file_path = ARTIFACTS_DIR / file_name
        
        df = EventSnapshot._create_event_df(events)
        df.to_parquet(file_path, index=False)
        print(f"Event snapshot dumped to: {file_path}")
        return file_path

    @staticmethod
    def load_events(file_path: Path) -> List[Dict[str, Any]]:
        """
        Loads an event stream from a parquet file.
        """
        if not file_path.exists():
            raise FileNotFoundError(f"Event snapshot file not found: {file_path}")
        
        df = pd.read_parquet(file_path)
        return df.to_dict(orient='records')

    @staticmethod
    def compare_event_streams(events1: List[Dict[str, Any]], events2: List[Dict[str, Any]]) -> bool:
        """
        Compares two event streams for exact equality.
        This is crucial for golden snapshot testing.
        """
        df1 = EventSnapshot._create_event_df(events1)
        df2 = EventSnapshot._create_event_df(events2)

        # Basic check: same number of rows and columns
        if df1.shape != df2.shape:
            print(f"Shape mismatch: {df1.shape} vs {df2.shape}")
            return False
        
        # Advanced check: compare content ignoring potential minor float differences
        # For exact reproducibility, floating point numbers should be identical too.
        # Using .equals() for robust DataFrame comparison.
        are_equal = df1.equals(df2)
        
        if not are_equal:
            print("Event stream mismatch detected.")
            # For debugging, you might want to show differences:
            # diff = df1.compare(df2) # Requires pandas >= 1.1
            # print("Differences:\n", diff)
        
        return are_equal
    
    @staticmethod
    def generate_event_stream_hash(events: List[Dict[str, Any]]) -> str:
        """
        Generates a deterministic hash of an event stream.
        This is useful for quick integrity checks without loading full data.
        """
        if not events:
            return "empty_event_stream"
        
        df = EventSnapshot._create_event_df(events)
        # Convert DataFrame to a canonical string representation before hashing
        # This requires careful handling for consistent serialization (e.g., sort columns, sort rows)
    
    @classmethod
    def log_llm_interaction(
        cls,
        prompt_hash: str,
        model: str,
        temperature: float,
        cache_hit: bool,
        response_hash: str,
        deterministic_mode: bool,
        validation_passed: bool,
        response_time_ms: float
    ):
        """
        Log an LLM interaction for inclusion in snapshots.
        
        Args:
            prompt_hash: Hash of the input prompt
            model: Model name used
            temperature: Sampling temperature
            cache_hit: Whether response came from cache
            response_hash: Hash of the response
            deterministic_mode: Whether operating in deterministic mode
            validation_passed: Whether response validation passed
            response_time_ms: Response time in milliseconds
        """
        interaction = LLMInteractionLog(
            timestamp=datetime.datetime.now(datetime.timezone.utc).isoformat(),
            prompt_hash=prompt_hash,
            model=model,
            temperature=temperature,
            cache_hit=cache_hit,
            response_hash=response_hash,
            deterministic_mode=deterministic_mode,
            validation_passed=validation_passed,
            response_time_ms=response_time_ms
        )
        
        cls._llm_interactions.append(interaction)
        logger.debug(f"Logged LLM interaction: {prompt_hash[:16]}... (cache_hit: {cache_hit})")
    
    @classmethod
    def set_snapshot_metadata(
        cls,
        simulation_mode: str,
        master_seed: Optional[int] = None,
        llm_cache_status: Optional[Dict[str, Any]] = None,
        determinism_validation: Optional[Dict[str, Any]] = None,
        reproducibility_features_enabled: Optional[Dict[str, bool]] = None
    ):
        """
        Set enhanced metadata for snapshots.
        
        Args:
            simulation_mode: Current simulation mode
            master_seed: Master seed if set
            llm_cache_status: LLM cache statistics
            determinism_validation: Determinism validation results
            reproducibility_features_enabled: Status of reproducibility features
        """
        cls._snapshot_metadata = SnapshotMetadata(
            simulation_mode=simulation_mode,
            master_seed=master_seed,
            llm_cache_status=llm_cache_status or {},
            determinism_validation=determinism_validation or {},
            reproducibility_features_enabled=reproducibility_features_enabled or {}
        )
        
        logger.debug(f"Set snapshot metadata: mode={simulation_mode}, seed={master_seed}")
    
    @classmethod
    def dump_events_with_metadata(
        cls,
        events: List[Dict[str, Any]], 
        git_sha: str, 
        run_id: str,
        include_llm_interactions: bool = True,
        include_reproducibility_metadata: bool = True
    ) -> Optional[Path]:
        """
        Enhanced event dumping with LLM interactions and reproducibility metadata.
        
        Args:
            events: Event stream to dump
            git_sha: Git SHA for file naming
            run_id: Run ID for file naming
            include_llm_interactions: Whether to include LLM interaction logs
            include_reproducibility_metadata: Whether to include reproducibility metadata
            
        Returns:
            Path to created snapshot file
        """
        if not events:
            logger.warning("No events to dump. Skipping snapshot creation.")
            return None
        
        # Prepare enhanced event data
        enhanced_data = {
            "events": events,
            "snapshot_metadata": {
                "created_at": datetime.datetime.now(datetime.timezone.utc).isoformat(),
                "git_sha": git_sha,
                "run_id": run_id,
                "event_count": len(events),
                "snapshot_version": "2.0"
            }
        }
        
        # Add LLM interactions if requested
        if include_llm_interactions and cls._llm_interactions:
            enhanced_data["llm_interactions"] = [
                asdict(interaction) for interaction in cls._llm_interactions
            ]
            enhanced_data["snapshot_metadata"]["llm_interaction_count"] = len(cls._llm_interactions)
        
        # Add reproducibility metadata if available
        if include_reproducibility_metadata and cls._snapshot_metadata:
            enhanced_data["reproducibility_metadata"] = asdict(cls._snapshot_metadata)
        
        # Save to both standard format (for compatibility) and enhanced format
        file_name = f"{git_sha}_{run_id}.parquet"
        enhanced_file_name = f"{git_sha}_{run_id}_enhanced.json"
        
        standard_path = ARTIFACTS_DIR / file_name
        enhanced_path = ARTIFACTS_DIR / enhanced_file_name
        
        try:
            # Save standard parquet format (events only)
            df = cls._create_event_df(events)
            df.to_parquet(standard_path, index=False)
            
            # Save enhanced JSON format (everything)
            with open(enhanced_path, 'w') as f:
                json.dump(enhanced_data, f, indent=2, separators=(',', ': '))
            
            logger.info(f"Enhanced snapshot saved: {enhanced_path}")
            logger.info(f"Standard snapshot saved: {standard_path}")
            
            return enhanced_path
            
        except Exception as e:
            logger.error(f"Failed to save enhanced snapshot: {e}")
            # Fall back to standard dump
            return cls.dump_events(events, git_sha, run_id)
    
    @classmethod
    def load_enhanced_snapshot(cls, file_path: Path) -> Dict[str, Any]:
        """
        Load enhanced snapshot with all metadata.
        
        Args:
            file_path: Path to enhanced snapshot file
            
        Returns:
            Complete snapshot data including metadata
        """
        if not file_path.exists():
            raise FileNotFoundError(f"Enhanced snapshot file not found: {file_path}")
        
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            logger.info(f"Loaded enhanced snapshot: {file_path}")
            return data
            
        except Exception as e:
            logger.error(f"Failed to load enhanced snapshot: {e}")
            raise
    
    @classmethod
    def validate_snapshot_reproducibility(
        cls,
        snapshot1_path: Path,
        snapshot2_path: Path,
        tolerance_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Validate reproducibility between two snapshots.
        
        Args:
            snapshot1_path: Path to first snapshot
            snapshot2_path: Path to second snapshot
            tolerance_config: Tolerance configuration for comparisons
            
        Returns:
            Validation result with detailed analysis
        """
        try:
            # Load both snapshots
            snapshot1 = cls.load_enhanced_snapshot(snapshot1_path)
            snapshot2 = cls.load_enhanced_snapshot(snapshot2_path)
            
            validation_result = {
                "is_reproducible": True,
                "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(),
                "snapshot1_path": str(snapshot1_path),
                "snapshot2_path": str(snapshot2_path),
                "issues": [],
                "warnings": [],
                "statistics": {}
            }
            
            # Compare events
            events1 = snapshot1.get("events", [])
            events2 = snapshot2.get("events", [])
            
            events_match = cls.compare_event_streams(events1, events2)
            validation_result["events_match"] = events_match
            
            if not events_match:
                validation_result["is_reproducible"] = False
                validation_result["issues"].append("Event streams do not match")
            
            # Compare LLM interactions if available
            llm1 = snapshot1.get("llm_interactions", [])
            llm2 = snapshot2.get("llm_interactions", [])
            
            if llm1 and llm2:
                llm_match = cls._compare_llm_interactions(llm1, llm2, tolerance_config)
                validation_result["llm_interactions_match"] = llm_match
                
                if not llm_match:
                    validation_result["is_reproducible"] = False
                    validation_result["issues"].append("LLM interactions do not match")
            
            # Compare reproducibility metadata
            meta1 = snapshot1.get("reproducibility_metadata", {})
            meta2 = snapshot2.get("reproducibility_metadata", {})
            
            if meta1 and meta2:
                # Compare key reproducibility settings
                key_fields = ["simulation_mode", "master_seed"]
                for field in key_fields:
                    if meta1.get(field) != meta2.get(field):
                        validation_result["warnings"].append(f"Reproducibility metadata differs: {field}")
            
            # Generate statistics
            validation_result["statistics"] = {
                "snapshot1_events": len(events1),
                "snapshot2_events": len(events2),
                "snapshot1_llm_interactions": len(llm1),
                "snapshot2_llm_interactions": len(llm2),
                "events_hash_1": cls.generate_event_stream_hash(events1),
                "events_hash_2": cls.generate_event_stream_hash(events2)
            }
            
            logger.info(f"Snapshot validation: {'PASSED' if validation_result['is_reproducible'] else 'FAILED'}")
            
            return validation_result
            
        except Exception as e:
            logger.error(f"Snapshot validation failed: {e}")
            return {
                "is_reproducible": False,
                "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(),
                "error": str(e),
                "issues": [f"Validation error: {e}"]
            }
    
    @classmethod
    def _compare_llm_interactions(
        cls,
        interactions1: List[Dict[str, Any]],
        interactions2: List[Dict[str, Any]],
        tolerance_config: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        Compare LLM interaction logs between snapshots.
        
        Args:
            interactions1: First set of LLM interactions
            interactions2: Second set of LLM interactions
            tolerance_config: Tolerance for comparisons
            
        Returns:
            True if interactions match within tolerance
        """
        if len(interactions1) != len(interactions2):
            logger.warning(f"LLM interaction count mismatch: {len(interactions1)} vs {len(interactions2)}")
            return False
        
        tolerance = tolerance_config or {}
        time_tolerance_ms = tolerance.get("time_tolerance_ms", 100.0)
        
        for i, (int1, int2) in enumerate(zip(interactions1, interactions2)):
            # Compare critical fields
            critical_fields = ["prompt_hash", "model", "temperature", "response_hash", "deterministic_mode"]
            
            for field in critical_fields:
                if int1.get(field) != int2.get(field):
                    logger.warning(f"LLM interaction {i} field mismatch: {field}")
                    return False
            
            # Compare response time with tolerance
            time1 = int1.get("response_time_ms", 0)
            time2 = int2.get("response_time_ms", 0)
            
            if abs(time1 - time2) > time_tolerance_ms:
                logger.warning(f"LLM interaction {i} response time difference: {abs(time1 - time2)}ms")
                # Don't fail on timing differences, just warn
        
        return True
    
    @classmethod
    def clear_llm_interactions(cls):
        """Clear accumulated LLM interaction logs."""
        cls._llm_interactions.clear()
        logger.debug("Cleared LLM interaction logs")
    
    @classmethod
    def get_llm_interaction_summary(cls) -> Dict[str, Any]:
        """
        Get summary of accumulated LLM interactions.
        
        Returns:
            Summary statistics of LLM interactions
        """
        if not cls._llm_interactions:
            return {"total_interactions": 0}
        
        cache_hits = sum(1 for i in cls._llm_interactions if i.cache_hit)
        deterministic_calls = sum(1 for i in cls._llm_interactions if i.deterministic_mode)
        validation_failures = sum(1 for i in cls._llm_interactions if not i.validation_passed)
        
        avg_response_time = sum(i.response_time_ms for i in cls._llm_interactions) / len(cls._llm_interactions)
        
        models_used = set(i.model for i in cls._llm_interactions)
        
        return {
            "total_interactions": len(cls._llm_interactions),
            "cache_hits": cache_hits,
            "cache_hit_ratio": cache_hits / len(cls._llm_interactions),
            "deterministic_calls": deterministic_calls,
            "validation_failures": validation_failures,
            "average_response_time_ms": avg_response_time,
            "models_used": list(models_used),
            "first_interaction": cls._llm_interactions[0].timestamp if cls._llm_interactions else None,
            "last_interaction": cls._llm_interactions[-1].timestamp if cls._llm_interactions else None
        }
        canonical_string = df.sort_values(by=df.columns.tolist()).to_json(orient='records', sort_keys=True)
        return hashlib.sha256(canonical_string.encode()).hexdigest()[:16]