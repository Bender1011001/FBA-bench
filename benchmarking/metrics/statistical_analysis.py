"""
Statistical analysis framework for FBA-Bench.

This module provides advanced statistical methods for analyzing benchmarking results,
including confidence interval calculations, statistical significance testing, effect size
measurement, correlation analysis, trend analysis, anomaly detection, and predictive modeling.
"""

import math
import statistics
from typing import Dict, Any, List, Optional, Tuple, Union, Callable
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
import numpy as np
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

from .base import BaseMetric, MetricConfig


class StatisticalTest(Enum):
    """Statistical test types."""
    T_TEST = "t_test"
    PAIRED_T_TEST = "paired_t_test"
    MANN_WHITNEY_U = "mann_whitney_u"
    ANOVA = "anova"
    CHI_SQUARE = "chi_square"
    PEARSON_CORRELATION = "pearson_correlation"
    SPEARMAN_CORRELATION = "spearman_correlation"
    KOLMOGOROV_SMIRNOV = "kolmogorov_smirnov"


@dataclass
class StatisticalResult:
    """Result of a statistical test."""
    test_name: str
    statistic: float
    p_value: float
    confidence_interval: Optional[Tuple[float, float]] = None
    effect_size: Optional[float] = None
    interpretation: str = ""
    significant: bool = False
    alpha: float = 0.05


@dataclass
class TrendAnalysis:
    """Trend analysis results."""
    trend_direction: str  # "increasing", "decreasing", "stable"
    trend_strength: float
    trend_significance: float
    slope: float
    r_squared: float
    prediction_interval: Tuple[float, float]


@dataclass
class AnomalyDetection:
    """Anomaly detection results."""
    anomaly_indices: List[int]
    anomaly_scores: List[float]
    anomaly_threshold: float
    num_anomalies: int
    anomaly_severity: Dict[str, float]


@dataclass
class PredictionModel:
    """Prediction model results."""
    model_type: str
    predictions: List[float]
    confidence_intervals: List[Tuple[float, float]]
    model_accuracy: float
    feature_importance: Dict[str, float]
    training_error: float
    validation_error: float


class StatisticalAnalysisFramework(BaseMetric):
    """
    Advanced statistical analysis framework for benchmarking results.
    
    This class provides comprehensive statistical analysis capabilities including
    confidence interval calculations, statistical significance testing, effect size
    measurement, correlation analysis, trend analysis, anomaly detection, and
    predictive modeling.
    """
    
    def __init__(self, config: MetricConfig = None):
        """
        Initialize statistical analysis framework.
        
        Args:
            config: Metric configuration (uses defaults if None)
        """
        if config is None:
            config = MetricConfig(
                name="statistical_analysis",
                description="Statistical analysis framework",
                unit="score",
                min_value=0.0,
                max_value=100.0,
                target_value=90.0
            )
        
        super().__init__(config)
        self.alpha = 0.05  # Significance level
        self.confidence_level = 0.95  # Confidence level
        
        # Sub-metric configurations
        self.confidence_interval_config = MetricConfig(
            name="confidence_interval",
            description="Confidence interval calculations",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.significance_testing_config = MetricConfig(
            name="significance_testing",
            description="Statistical significance testing",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.effect_size_config = MetricConfig(
            name="effect_size",
            description="Effect size measurement",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.correlation_analysis_config = MetricConfig(
            name="correlation_analysis",
            description="Correlation analysis between metrics",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.trend_analysis_config = MetricConfig(
            name="trend_analysis",
            description="Trend analysis capabilities",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.anomaly_detection_config = MetricConfig(
            name="anomaly_detection",
            description="Anomaly detection algorithms",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.predictive_modeling_config = MetricConfig(
            name="predictive_modeling",
            description="Predictive modeling for performance",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
    
    def calculate(self, data: Dict[str, Any]) -> float:
        """
        Calculate statistical analysis score.
        
        Args:
            data: Data containing statistical analysis metrics
            
        Returns:
            Overall statistical analysis score
        """
        # Calculate sub-metric scores
        confidence_interval = self.calculate_confidence_interval_score(data)
        significance_testing = self.calculate_significance_testing_score(data)
        effect_size = self.calculate_effect_size_score(data)
        correlation_analysis = self.calculate_correlation_analysis_score(data)
        trend_analysis = self.calculate_trend_analysis_score(data)
        anomaly_detection = self.calculate_anomaly_detection_score(data)
        predictive_modeling = self.calculate_predictive_modeling_score(data)
        
        # Calculate weighted average
        weights = {
            'confidence_interval': 0.15,
            'significance_testing': 0.18,
            'effect_size': 0.15,
            'correlation_analysis': 0.15,
            'trend_analysis': 0.12,
            'anomaly_detection': 0.12,
            'predictive_modeling': 0.13
        }
        
        overall_score = (
            confidence_interval * weights['confidence_interval'] +
            significance_testing * weights['significance_testing'] +
            effect_size * weights['effect_size'] +
            correlation_analysis * weights['correlation_analysis'] +
            trend_analysis * weights['trend_analysis'] +
            anomaly_detection * weights['anomaly_detection'] +
            predictive_modeling * weights['predictive_modeling']
        )
        
        return overall_score
    
    def calculate_confidence_interval_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate confidence interval score.
        
        Args:
            data: Data containing confidence interval metrics
            
        Returns:
            Confidence interval score
        """
        interval_data = data.get('confidence_interval_data', [])
        if not interval_data:
            return 0.0
        
        interval_scores = []
        
        for interval_info in interval_data:
            # Evaluate interval components
            interval_width = interval_info.get('interval_width', 0.0)
            coverage_rate = interval_info.get('coverage_rate', 0.0)
            precision_score = interval_info.get('precision_score', 0.0)
            
            # Calculate weighted interval score
            weights = {
                'interval_width': 0.3,
                'coverage_rate': 0.4,
                'precision_score': 0.3
            }
            
            # Normalize scores
            width_score = max(0.0, 1.0 - interval_width / 2.0)  # Narrower intervals are better
            coverage_score = coverage_rate  # Higher coverage is better
            precision_normalized = precision_score  # Higher precision is better
            
            interval_score = (
                width_score * weights['interval_width'] +
                coverage_score * weights['coverage_rate'] +
                precision_normalized * weights['precision_score']
            )
            
            interval_scores.append(interval_score)
        
        return statistics.mean(interval_scores) * 100
    
    def calculate_significance_testing_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate significance testing score.
        
        Args:
            data: Data containing significance testing metrics
            
        Returns:
            Significance testing score
        """
        test_results = data.get('test_results', [])
        if not test_results:
            return 0.0
        
        test_scores = []
        
        for result in test_results:
            # Evaluate test components
            test_power = result.get('test_power', 0.0)
            p_value_accuracy = result.get('p_value_accuracy', 0.0)
            assumption_validity = result.get('assumption_validity', 0.0)
            
            # Calculate weighted test score
            weights = {
                'test_power': 0.4,
                'p_value_accuracy': 0.3,
                'assumption_validity': 0.3
            }
            
            test_score = (
                test_power * weights['test_power'] +
                p_value_accuracy * weights['p_value_accuracy'] +
                assumption_validity * weights['assumption_validity']
            )
            
            test_scores.append(test_score)
        
        return statistics.mean(test_scores) * 100
    
    def calculate_effect_size_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate effect size score.
        
        Args:
            data: Data containing effect size metrics
            
        Returns:
            Effect size score
        """
        effect_sizes = data.get('effect_sizes', [])
        if not effect_sizes:
            return 0.0
        
        size_scores = []
        
        for size_info in effect_sizes:
            # Evaluate effect size components
            cohen_d = size_info.get('cohen_d', 0.0)
            eta_squared = size_info.get('eta_squared', 0.0)
            odds_ratio = size_info.get('odds_ratio', 1.0)
            
            # Calculate weighted size score
            weights = {
                'cohen_d': 0.4,
                'eta_squared': 0.3,
                'odds_ratio': 0.3
            }
            
            # Normalize scores
            d_score = min(1.0, abs(cohen_d) / 2.0)  # Cohen's d of 2.0 is considered large
            eta_score = min(1.0, eta_squared * 5)  # Eta squared of 0.2 is considered large
            odds_score = min(1.0, abs(math.log(odds_ratio)) / 2.0)  # Log odds ratio of 2.0 is considered large
            
            size_score = (
                d_score * weights['cohen_d'] +
                eta_score * weights['eta_squared'] +
                odds_score * weights['odds_ratio']
            )
            
            size_scores.append(size_score)
        
        return statistics.mean(size_scores) * 100
    
    def calculate_correlation_analysis_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate correlation analysis score.
        
        Args:
            data: Data containing correlation analysis metrics
            
        Returns:
            Correlation analysis score
        """
        correlations = data.get('correlations', [])
        if not correlations:
            return 0.0
        
        correlation_scores = []
        
        for corr_info in correlations:
            # Evaluate correlation components
            pearson_r = corr_info.get('pearson_r', 0.0)
            spearman_rho = corr_info.get('spearman_rho', 0.0)
            significance = corr_info.get('significance', 0.0)
            
            # Calculate weighted correlation score
            weights = {
                'pearson_r': 0.4,
                'spearman_rho': 0.3,
                'significance': 0.3
            }
            
            # Normalize scores
            pearson_score = abs(pearson_r)  # Higher absolute correlation is better
            spearman_score = abs(spearman_rho)  # Higher absolute correlation is better
            significance_score = significance  # Higher significance is better
            
            correlation_score = (
                pearson_score * weights['pearson_r'] +
                spearman_score * weights['spearman_rho'] +
                significance_score * weights['significance']
            )
            
            correlation_scores.append(correlation_score)
        
        return statistics.mean(correlation_scores) * 100
    
    def calculate_trend_analysis_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate trend analysis score.
        
        Args:
            data: Data containing trend analysis metrics
            
        Returns:
            Trend analysis score
        """
        trends = data.get('trends', [])
        if not trends:
            return 0.0
        
        trend_scores = []
        
        for trend_info in trends:
            # Evaluate trend components
            trend_strength = trend_info.get('trend_strength', 0.0)
            trend_significance = trend_info.get('trend_significance', 0.0)
            prediction_accuracy = trend_info.get('prediction_accuracy', 0.0)
            
            # Calculate weighted trend score
            weights = {
                'trend_strength': 0.35,
                'trend_significance': 0.35,
                'prediction_accuracy': 0.3
            }
            
            trend_score = (
                trend_strength * weights['trend_strength'] +
                trend_significance * weights['trend_significance'] +
                prediction_accuracy * weights['prediction_accuracy']
            )
            
            trend_scores.append(trend_score)
        
        return statistics.mean(trend_scores) * 100
    
    def calculate_anomaly_detection_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate anomaly detection score.
        
        Args:
            data: Data containing anomaly detection metrics
            
        Returns:
            Anomaly detection score
        """
        anomaly_results = data.get('anomaly_results', [])
        if not anomaly_results:
            return 0.0
        
        anomaly_scores = []
        
        for result in anomaly_results:
            # Evaluate anomaly detection components
            detection_rate = result.get('detection_rate', 0.0)
            false_positive_rate = result.get('false_positive_rate', 0.0)
            anomaly_severity = result.get('anomaly_severity', 0.0)
            
            # Calculate weighted anomaly score
            weights = {
                'detection_rate': 0.5,
                'false_positive_rate': 0.3,
                'anomaly_severity': 0.2
            }
            
            # Normalize scores
            detection_score = detection_rate  # Higher detection rate is better
            fp_score = 1.0 - false_positive_rate  # Lower false positive rate is better
            severity_score = anomaly_severity  # Higher severity detection is better
            
            anomaly_score = (
                detection_score * weights['detection_rate'] +
                fp_score * weights['false_positive_rate'] +
                severity_score * weights['anomaly_severity']
            )
            
            anomaly_scores.append(anomaly_score)
        
        return statistics.mean(anomaly_scores) * 100
    
    def calculate_predictive_modeling_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate predictive modeling score.
        
        Args:
            data: Data containing predictive modeling metrics
            
        Returns:
            Predictive modeling score
        """
        models = data.get('models', [])
        if not models:
            return 0.0
        
        model_scores = []
        
        for model in models:
            # Evaluate model components
            model_accuracy = model.get('model_accuracy', 0.0)
            generalization_score = model.get('generalization_score', 0.0)
            feature_importance = model.get('feature_importance', {})
            
            # Calculate weighted model score
            weights = {
                'model_accuracy': 0.5,
                'generalization_score': 0.3,
                'feature_importance': 0.2
            }
            
            # Normalize scores
            accuracy_score = model_accuracy  # Higher accuracy is better
            generalization_normalized = generalization_score  # Higher generalization is better
            importance_score = len(feature_importance) / 10.0  # More features considered is better
            importance_score = min(1.0, importance_score)
            
            model_score = (
                accuracy_score * weights['model_accuracy'] +
                generalization_normalized * weights['generalization_score'] +
                importance_score * weights['feature_importance']
            )
            
            model_scores.append(model_score)
        
        return statistics.mean(model_scores) * 100
    
    def calculate_confidence_interval(
        self, 
        data: List[float], 
        confidence: float = None
    ) -> Tuple[float, float]:
        """
        Calculate confidence interval for data.
        
        Args:
            data: Data points
            confidence: Confidence level (uses instance default if None)
            
        Returns:
            Tuple of (lower_bound, upper_bound)
        """
        if not data:
            return (0.0, 0.0)
        
        if confidence is None:
            confidence = self.confidence_level
        
        n = len(data)
        mean = statistics.mean(data)
        std_err = statistics.stdev(data) / math.sqrt(n)
        
        # Calculate critical value
        if n >= 30:
            # Use z-score for large samples
            critical_value = stats.norm.ppf((1 + confidence) / 2)
        else:
            # Use t-score for small samples
            critical_value = stats.t.ppf((1 + confidence) / 2, n - 1)
        
        margin_of_error = critical_value * std_err
        
        return (mean - margin_of_error, mean + margin_of_error)
    
    def perform_statistical_test(
        self,
        test_type: StatisticalTest,
        sample1: List[float],
        sample2: List[float] = None,
        **kwargs
    ) -> StatisticalResult:
        """
        Perform statistical test.
        
        Args:
            test_type: Type of statistical test
            sample1: First sample data
            sample2: Second sample data (for two-sample tests)
            **kwargs: Additional test-specific parameters
            
        Returns:
            Statistical test result
        """
        if test_type == StatisticalTest.T_TEST:
            return self._perform_t_test(sample1, sample2, **kwargs)
        elif test_type == StatisticalTest.PAIRED_T_TEST:
            return self._perform_paired_t_test(sample1, sample2, **kwargs)
        elif test_type == StatisticalTest.MANN_WHITNEY_U:
            return self._perform_mann_whitney_u_test(sample1, sample2, **kwargs)
        elif test_type == StatisticalTest.ANOVA:
            return self._perform_anova_test(sample1, sample2, **kwargs)
        elif test_type == StatisticalTest.CHI_SQUARE:
            return self._perform_chi_square_test(sample1, sample2, **kwargs)
        elif test_type == StatisticalTest.PEARSON_CORRELATION:
            return self._perform_pearson_correlation(sample1, sample2, **kwargs)
        elif test_type == StatisticalTest.SPEARMAN_CORRELATION:
            return self._perform_spearman_correlation(sample1, sample2, **kwargs)
        elif test_type == StatisticalTest.KOLMOGOROV_SMIRNOV:
            return self._perform_kolmogorov_smirnov_test(sample1, sample2, **kwargs)
        else:
            raise ValueError(f"Unsupported test type: {test_type}")
    
    def _perform_t_test(
        self, 
        sample1: List[float], 
        sample2: List[float], 
        **kwargs
    ) -> StatisticalResult:
        """Perform independent t-test."""
        if not sample1 or not sample2:
            raise ValueError("Sample lists cannot be empty")
        
        statistic, p_value = stats.ttest_ind(sample1, sample2)
        
        # Calculate effect size (Cohen's d)
        mean1, mean2 = statistics.mean(sample1), statistics.mean(sample2)
        var1, var2 = statistics.variance(sample1), statistics.variance(sample2)
        n1, n2 = len(sample1), len(sample2)
        
        pooled_std = math.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))
        effect_size = (mean1 - mean2) / pooled_std if pooled_std > 0 else 0.0
        
        # Calculate confidence interval
        mean_diff = mean1 - mean2
        std_diff = math.sqrt(var1/n1 + var2/n2)
        n = min(n1, n2)
        
        if std_diff > 0 and n > 1:
            if n >= 30:
                critical_value = stats.norm.ppf(1 - self.alpha/2)
            else:
                critical_value = stats.t.ppf(1 - self.alpha/2, n - 1)
            
            margin_of_error = critical_value * std_diff
            confidence_interval = (mean_diff - margin_of_error, mean_diff + margin_of_error)
        else:
            confidence_interval = None
        
        # Interpret result
        significant = p_value < self.alpha
        if significant:
            interpretation = f"Significant difference detected (p={p_value:.4f})"
        else:
            interpretation = f"No significant difference detected (p={p_value:.4f})"
        
        return StatisticalResult(
            test_name=StatisticalTest.T_TEST.value,
            statistic=statistic,
            p_value=p_value,
            confidence_interval=confidence_interval,
            effect_size=effect_size,
            interpretation=interpretation,
            significant=significant,
            alpha=self.alpha
        )
    
    def _perform_paired_t_test(
        self, 
        sample1: List[float], 
        sample2: List[float], 
        **kwargs
    ) -> StatisticalResult:
        """Perform paired t-test."""
        if not sample1 or not sample2:
            raise ValueError("Sample lists cannot be empty")
        
        if len(sample1) != len(sample2):
            raise ValueError("Paired samples must have the same length")
        
        statistic, p_value = stats.ttest_rel(sample1, sample2)
        
        # Calculate effect size (Cohen's d for paired samples)
        differences = [s1 - s2 for s1, s2 in zip(sample1, sample2)]
        mean_diff = statistics.mean(differences)
        std_diff = statistics.stdev(differences) if len(differences) > 1 else 1.0
        effect_size = mean_diff / std_diff if std_diff > 0 else 0.0
        
        # Calculate confidence interval
        n = len(sample1)
        if std_diff > 0 and n > 1:
            if n >= 30:
                critical_value = stats.norm.ppf(1 - self.alpha/2)
            else:
                critical_value = stats.t.ppf(1 - self.alpha/2, n - 1)
            
            margin_of_error = critical_value * (std_diff / math.sqrt(n))
            confidence_interval = (mean_diff - margin_of_error, mean_diff + margin_of_error)
        else:
            confidence_interval = None
        
        # Interpret result
        significant = p_value < self.alpha
        if significant:
            interpretation = f"Significant difference detected (p={p_value:.4f})"
        else:
            interpretation = f"No significant difference detected (p={p_value:.4f})"
        
        return StatisticalResult(
            test_name=StatisticalTest.PAIRED_T_TEST.value,
            statistic=statistic,
            p_value=p_value,
            confidence_interval=confidence_interval,
            effect_size=effect_size,
            interpretation=interpretation,
            significant=significant,
            alpha=self.alpha
        )
    
    def _perform_mann_whitney_u_test(
        self, 
        sample1: List[float], 
        sample2: List[float], 
        **kwargs
    ) -> StatisticalResult:
        """Perform Mann-Whitney U test."""
        if not sample1 or not sample2:
            raise ValueError("Sample lists cannot be empty")
        
        statistic, p_value = stats.mannwhitneyu(sample1, sample2, alternative='two-sided')
        
        # Calculate effect size (r = Z / sqrt(N))
        n1, n2 = len(sample1), len(sample2)
        z_score = stats.norm.ppf(1 - p_value/2) if p_value > 0 else 0.0
        effect_size = abs(z_score) / math.sqrt(n1 + n2)
        
        # Interpret result
        significant = p_value < self.alpha
        if significant:
            interpretation = f"Significant difference detected (p={p_value:.4f})"
        else:
            interpretation = f"No significant difference detected (p={p_value:.4f})"
        
        return StatisticalResult(
            test_name=StatisticalTest.MANN_WHITNEY_U.value,
            statistic=statistic,
            p_value=p_value,
            effect_size=effect_size,
            interpretation=interpretation,
            significant=significant,
            alpha=self.alpha
        )
    
    def _perform_anova_test(
        self, 
        sample1: List[float], 
        sample2: List[float], 
        **kwargs
    ) -> StatisticalResult:
        """Perform one-way ANOVA test."""
        additional_samples = kwargs.get('additional_samples', [])
        samples = [sample1, sample2] + additional_samples
        
        if len(samples) < 2:
            raise ValueError("At least two samples are required for ANOVA")
        
        if any(not sample for sample in samples):
            raise ValueError("Sample lists cannot be empty")
        
        statistic, p_value = stats.f_oneway(*samples)
        
        # Calculate effect size (eta-squared)
        total_variance = statistics.variance([val for sample in samples for val in sample])
        between_group_variance = sum(
            len(sample) * (statistics.mean(sample) - statistics.mean([val for sample in samples for val in sample]))**2
            for sample in samples
        ) / sum(len(sample) for sample in samples)
        
        effect_size = between_group_variance / total_variance if total_variance > 0 else 0.0
        
        # Interpret result
        significant = p_value < self.alpha
        if significant:
            interpretation = f"Significant difference detected between groups (p={p_value:.4f})"
        else:
            interpretation = f"No significant difference detected between groups (p={p_value:.4f})"
        
        return StatisticalResult(
            test_name=StatisticalTest.ANOVA.value,
            statistic=statistic,
            p_value=p_value,
            effect_size=effect_size,
            interpretation=interpretation,
            significant=significant,
            alpha=self.alpha
        )
    
    def _perform_chi_square_test(
        self, 
        sample1: List[float], 
        sample2: List[float], 
        **kwargs
    ) -> StatisticalResult:
        """Perform chi-square goodness-of-fit test."""
        observed = sample1
        expected = sample2
        
        if not observed or not expected:
            raise ValueError("Observed and expected lists cannot be empty")
        
        if len(observed) != len(expected):
            raise ValueError("Observed and expected lists must have the same length")
        
        statistic, p_value = stats.chisquare(observed, expected)
        
        # Calculate effect size (Cramer's V)
        n = sum(observed)
        k = len(observed)
        cramers_v = math.sqrt(statistic / (n * (k - 1))) if n > 0 and k > 1 else 0.0
        
        # Interpret result
        significant = p_value < self.alpha
        if significant:
            interpretation = f"Significant deviation from expected distribution (p={p_value:.4f})"
        else:
            interpretation = f"No significant deviation from expected distribution (p={p_value:.4f})"
        
        return StatisticalResult(
            test_name=StatisticalTest.CHI_SQUARE.value,
            statistic=statistic,
            p_value=p_value,
            effect_size=cramers_v,
            interpretation=interpretation,
            significant=significant,
            alpha=self.alpha
        )
    
    def _perform_pearson_correlation(
        self, 
        sample1: List[float], 
        sample2: List[float], 
        **kwargs
    ) -> StatisticalResult:
        """Perform Pearson correlation test."""
        if not sample1 or not sample2:
            raise ValueError("Sample lists cannot be empty")
        
        if len(sample1) != len(sample2):
            raise ValueError("Samples must have the same length")
        
        statistic, p_value = stats.pearsonr(sample1, sample2)
        
        # Effect size is the correlation coefficient itself
        effect_size = abs(statistic)
        
        # Interpret result
        significant = p_value < self.alpha
        if significant:
            interpretation = f"Significant correlation detected (r={statistic:.4f}, p={p_value:.4f})"
        else:
            interpretation = f"No significant correlation detected (r={statistic:.4f}, p={p_value:.4f})"
        
        return StatisticalResult(
            test_name=StatisticalTest.PEARSON_CORRELATION.value,
            statistic=statistic,
            p_value=p_value,
            effect_size=effect_size,
            interpretation=interpretation,
            significant=significant,
            alpha=self.alpha
        )
    
    def _perform_spearman_correlation(
        self, 
        sample1: List[float], 
        sample2: List[float], 
        **kwargs
    ) -> StatisticalResult:
        """Perform Spearman correlation test."""
        if not sample1 or not sample2:
            raise ValueError("Sample lists cannot be empty")
        
        if len(sample1) != len(sample2):
            raise ValueError("Samples must have the same length")
        
        statistic, p_value = stats.spearmanr(sample1, sample2)
        
        # Effect size is the correlation coefficient itself
        effect_size = abs(statistic)
        
        # Interpret result
        significant = p_value < self.alpha
        if significant:
            interpretation = f"Significant correlation detected (rho={statistic:.4f}, p={p_value:.4f})"
        else:
            interpretation = f"No significant correlation detected (rho={statistic:.4f}, p={p_value:.4f})"
        
        return StatisticalResult(
            test_name=StatisticalTest.SPEARMAN_CORRELATION.value,
            statistic=statistic,
            p_value=p_value,
            effect_size=effect_size,
            interpretation=interpretation,
            significant=significant,
            alpha=self.alpha
        )
    
    def _perform_kolmogorov_smirnov_test(
        self, 
        sample1: List[float], 
        sample2: List[float], 
        **kwargs
    ) -> StatisticalResult:
        """Perform Kolmogorov-Smirnov test."""
        if not sample1 or not sample2:
            raise ValueError("Sample lists cannot be empty")
        
        statistic, p_value = stats.ks_2samp(sample1, sample2)
        
        # Effect size is the KS statistic itself
        effect_size = statistic
        
        # Interpret result
        significant = p_value < self.alpha
        if significant:
            interpretation = f"Significant difference in distributions detected (p={p_value:.4f})"
        else:
            interpretation = f"No significant difference in distributions detected (p={p_value:.4f})"
        
        return StatisticalResult(
            test_name=StatisticalTest.KOLMOGOROV_SMIRNOV.value,
            statistic=statistic,
            p_value=p_value,
            effect_size=effect_size,
            interpretation=interpretation,
            significant=significant,
            alpha=self.alpha
        )
    
    def analyze_trend(self, data: List[float], timestamps: List[datetime] = None) -> TrendAnalysis:
        """
        Analyze trend in time series data.
        
        Args:
            data: Time series data
            timestamps: Timestamps for data points (uses sequential indices if None)
            
        Returns:
            Trend analysis results
        """
        if not data:
            return TrendAnalysis(
                trend_direction="stable",
                trend_strength=0.0,
                trend_significance=0.0,
                slope=0.0,
                r_squared=0.0,
                prediction_interval=(0.0, 0.0)
            )
        
        # Prepare data for regression
        if timestamps is None:
            x = np.array(range(len(data))).reshape(-1, 1)
        else:
            # Convert timestamps to numeric values
            base_time = timestamps[0]
            x = np.array([(t - base_time).total_seconds() for t in timestamps]).reshape(-1, 1)
        
        y = np.array(data)
        
        # Perform linear regression
        model = LinearRegression()
        model.fit(x, y)
        
        # Get predictions
        predictions = model.predict(x)
        
        # Calculate metrics
        slope = model.coef_[0]
        r_squared = r2_score(y, predictions)
        
        # Determine trend direction
        if abs(slope) < 1e-6:
            trend_direction = "stable"
        elif slope > 0:
            trend_direction = "increasing"
        else:
            trend_direction = "decreasing"
        
        # Calculate trend strength (normalized slope)
        if len(data) > 1:
            x_range = x.max() - x.min()
            if x_range > 0:
                normalized_slope = slope * x_range / (y.max() - y.min()) if y.max() != y.min() else 0.0
                trend_strength = min(1.0, abs(normalized_slope))
            else:
                trend_strength = 0.0
        else:
            trend_strength = 0.0
        
        # Calculate trend significance (simplified)
        n = len(data)
        if n > 2:
            # Calculate correlation coefficient and its significance
            correlation = np.corrcoef(x.flatten(), y)[0, 1]
            if abs(correlation) < 1.0:
                t_stat = correlation * math.sqrt((n - 2) / (1 - correlation**2))
                p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - 2))
                trend_significance = 1.0 - p_value
            else:
                trend_significance = 1.0
        else:
            trend_significance = 0.0
        
        # Calculate prediction interval (simplified)
        residuals = y - predictions
        mse = mean_squared_error(y, predictions)
        std_error = math.sqrt(mse)
        
        # 95% prediction interval
        prediction_interval = (
            predictions[-1] - 1.96 * std_error,
            predictions[-1] + 1.96 * std_error
        )
        
        return TrendAnalysis(
            trend_direction=trend_direction,
            trend_strength=trend_strength,
            trend_significance=trend_significance,
            slope=slope,
            r_squared=r_squared,
            prediction_interval=prediction_interval
        )
    
    def detect_anomalies(
        self, 
        data: List[float], 
        contamination: float = 0.1,
        method: str = "isolation_forest"
    ) -> AnomalyDetection:
        """
        Detect anomalies in data.
        
        Args:
            data: Data points
            contamination: Expected proportion of anomalies
            method: Anomaly detection method
            
        Returns:
            Anomaly detection results
        """
        if not data:
            return AnomalyDetection(
                anomaly_indices=[],
                anomaly_scores=[],
                anomaly_threshold=0.0,
                num_anomalies=0,
                anomaly_severity={}
            )
        
        # Prepare data
        x = np.array(data).reshape(-1, 1)
        
        if method == "isolation_forest":
            # Use Isolation Forest for anomaly detection
            model = IsolationForest(contamination=contamination, random_state=42)
            anomaly_labels = model.fit_predict(x)
            anomaly_scores = model.decision_function(x)
            
            # Identify anomalies
            anomaly_indices = [i for i, label in enumerate(anomaly_labels) if label == -1]
            anomaly_threshold = np.percentile(anomaly_scores, contamination * 100)
            
            # Calculate anomaly severity
            anomaly_severity = {}
            for idx in anomaly_indices:
                severity = abs(anomaly_scores[idx])
                anomaly_severity[f"point_{idx}"] = severity
            
            return AnomalyDetection(
                anomaly_indices=anomaly_indices,
                anomaly_scores=anomaly_scores.tolist(),
                anomaly_threshold=anomaly_threshold,
                num_anomalies=len(anomaly_indices),
                anomaly_severity=anomaly_severity
            )
        else:
            raise ValueError(f"Unsupported anomaly detection method: {method}")
    
    def build_prediction_model(
        self,
        features: List[List[float]],
        targets: List[float],
        model_type: str = "linear_regression",
        test_size: float = 0.2
    ) -> PredictionModel:
        """
        Build prediction model for performance.
        
        Args:
            features: Feature data
            targets: Target values
            model_type: Type of model to build
            test_size: Proportion of data for testing
            
        Returns:
            Prediction model results
        """
        if not features or not targets:
            return PredictionModel(
                model_type=model_type,
                predictions=[],
                confidence_intervals=[],
                model_accuracy=0.0,
                feature_importance={},
                training_error=0.0,
                validation_error=0.0
            )
        
        # Prepare data
        X = np.array(features)
        y = np.array(targets)
        
        # Split data for training and testing
        split_idx = int(len(X) * (1 - test_size))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        if model_type == "linear_regression":
            # Build linear regression model
            model = LinearRegression()
            model.fit(X_train, y_train)
            
            # Make predictions
            train_predictions = model.predict(X_train)
            test_predictions = model.predict(X_test)
            all_predictions = model.predict(X)
            
            # Calculate accuracy
            model_accuracy = r2_score(y_test, test_predictions)
            
            # Calculate errors
            training_error = mean_squared_error(y_train, train_predictions)
            validation_error = mean_squared_error(y_test, test_predictions)
            
            # Calculate feature importance (absolute coefficients)
            feature_importance = {}
            for i, coef in enumerate(model.coef_):
                feature_importance[f"feature_{i}"] = abs(coef)
            
            # Calculate confidence intervals (simplified)
            residuals = y - all_predictions
            std_error = math.sqrt(mean_squared_error(y, all_predictions))
            confidence_intervals = [
                (pred - 1.96 * std_error, pred + 1.96 * std_error)
                for pred in all_predictions
            ]
            
            return PredictionModel(
                model_type=model_type,
                predictions=all_predictions.tolist(),
                confidence_intervals=confidence_intervals,
                model_accuracy=model_accuracy,
                feature_importance=feature_importance,
                training_error=training_error,
                validation_error=validation_error
            )
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
    
    def calculate_power_analysis(
        self, 
        effect_size: float, 
        alpha: float = None, 
        power: float = 0.8,
        test_type: StatisticalTest = StatisticalTest.T_TEST
    ) -> Dict[str, Any]:
        """
        Calculate sample size needed for statistical power.
        
        Args:
            effect_size: Expected effect size
            alpha: Significance level (uses instance default if None)
            power: Desired statistical power
            test_type: Type of statistical test
            
        Returns:
            Dictionary with power analysis results
        """
        if alpha is None:
            alpha = self.alpha
        
        # Simplified power analysis calculation
        if effect_size <= 0:
            raise ValueError("Effect size must be positive")
        
        # Calculate required sample size using approximation
        if test_type in [StatisticalTest.T_TEST, StatisticalTest.PAIRED_T_TEST]:
            # For t-test
            if power >= 0.8:
                # Simplified approximation for common scenarios
                n_per_group = int(16 / (effect_size ** 2))
            else:
                # More general approximation
                n_per_group = int(8 / (effect_size ** 2))
            
            total_sample_size = 2 * n_per_group
        elif test_type == StatisticalTest.ANOVA:
            # For ANOVA (simplified)
            n_per_group = int(8 / (effect_size ** 2))
            total_sample_size = 3 * n_per_group  # Assuming 3 groups
        else:
            # Default approximation
            n_per_group = int(16 / (effect_size ** 2))
            total_sample_size = 2 * n_per_group
        
        return {
            "effect_size": effect_size,
            "alpha": alpha,
            "power": power,
            "test_type": test_type.value,
            "sample_size_per_group": n_per_group,
            "total_sample_size": total_sample_size
        }
    
    def calculate_correlation_matrix(self, data: Dict[str, List[float]]) -> Dict[str, Dict[str, float]]:
        """
        Calculate correlation matrix for multiple variables.
        
        Args:
            data: Dictionary of variable names to data lists
            
        Returns:
            Correlation matrix
        """
        variables = list(data.keys())
        correlation_matrix = {}
        
        for var1 in variables:
            correlation_matrix[var1] = {}
            for var2 in variables:
                if var1 == var2:
                    correlation_matrix[var1][var2] = 1.0
                else:
                    # Calculate Pearson correlation
                    try:
                        corr, _ = stats.pearsonr(data[var1], data[var2])
                        correlation_matrix[var1][var2] = corr
                    except:
                        correlation_matrix[var1][var2] = 0.0
        
        return correlation_matrix
