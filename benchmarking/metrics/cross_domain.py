"""
Cross-domain evaluation metrics for FBA-Bench.

This module provides metrics for evaluating agent performance across multiple domains,
including domain adaptation capability measures, knowledge transfer efficiency,
generalization ability assessment, and cross-domain consistency evaluation.
"""

import math
import statistics
from typing import Dict, Any, List, Optional, Tuple, Union, Set
from dataclasses import dataclass
from datetime import datetime
from collections import defaultdict

from .base import BaseMetric, MetricConfig


@dataclass
class DomainPerformance:
    """Performance metrics for a specific domain."""
    domain_name: str
    accuracy: float
    efficiency: float
    adaptation_time: float
    resource_usage: float
    domain_specific_metrics: Dict[str, float]


@dataclass
class DomainAdaptation:
    """Domain adaptation metrics."""
    source_domain: str
    target_domain: str
    adaptation_speed: float
    adaptation_quality: float
    knowledge_transfer_rate: float
    adaptation_efficiency: float


@dataclass
class KnowledgeTransfer:
    """Knowledge transfer metrics."""
    source_domains: List[str]
    target_domain: str
    transfer_effectiveness: float
    transfer_efficiency: float
    knowledge_retention: float
    transfer_bottlenecks: List[str]


@dataclass
class GeneralizationMetrics:
    """Generalization ability metrics."""
    training_domains: List[str]
    test_domains: List[str]
    generalization_score: float
    overfitting_measure: float
    robustness_score: float
    domain_gap_analysis: Dict[str, float]


class CrossDomainMetrics(BaseMetric):
    """
    Advanced metrics for evaluating cross-domain performance.
    
    This class provides comprehensive evaluation of agent performance across
    multiple domains, including domain adaptation, knowledge transfer,
    generalization ability, and cross-domain consistency.
    """
    
    def __init__(self, config: MetricConfig = None):
        """
        Initialize cross-domain metrics.
        
        Args:
            config: Metric configuration (uses defaults if None)
        """
        if config is None:
            config = MetricConfig(
                name="cross_domain_performance",
                description="Cross-domain evaluation score",
                unit="score",
                min_value=0.0,
                max_value=100.0,
                target_value=75.0
            )
        
        super().__init__(config)
        
        # Sub-metric configurations
        self.domain_adaptation_config = MetricConfig(
            name="domain_adaptation",
            description="Domain adaptation capability measures",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.knowledge_transfer_config = MetricConfig(
            name="knowledge_transfer",
            description="Knowledge transfer efficiency metrics",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.generalization_config = MetricConfig(
            name="generalization",
            description="Generalization ability assessment",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.consistency_config = MetricConfig(
            name="consistency",
            description="Cross-domain consistency evaluation",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
    
    def calculate(self, data: Dict[str, Any]) -> float:
        """
        Calculate cross-domain evaluation score.
        
        Args:
            data: Data containing cross-domain metrics
            
        Returns:
            Overall cross-domain score
        """
        # Calculate sub-metric scores
        domain_adaptation = self.calculate_domain_adaptation(data)
        knowledge_transfer = self.calculate_knowledge_transfer(data)
        generalization = self.calculate_generalization(data)
        consistency = self.calculate_consistency(data)
        
        # Calculate weighted average
        weights = {
            'domain_adaptation': 0.3,
            'knowledge_transfer': 0.25,
            'generalization': 0.25,
            'consistency': 0.2
        }
        
        overall_score = (
            domain_adaptation * weights['domain_adaptation'] +
            knowledge_transfer * weights['knowledge_transfer'] +
            generalization * weights['generalization'] +
            consistency * weights['consistency']
        )
        
        return overall_score

    # Aliases to match external/test API expectations
    def calculate_cross_domain_consistency(self, data: Dict[str, Any]) -> float:
        """Alias for calculate_consistency."""
        return self.calculate_consistency(data)

    def calculate_cross_domain_evaluation(self, data: Dict[str, Any]) -> float:
        """Alias for overall cross-domain evaluation (calculate)."""
        return self.calculate(data)

    def calculate_generalization_ability(self, data: Dict[str, Any]) -> float:
        """Alias for calculate_generalization."""
        return self.calculate_generalization(data)
    
    def calculate_domain_adaptation(self, data: Dict[str, Any]) -> float:
        """
        Calculate domain adaptation capability score.
        
        Args:
            data: Data containing domain adaptation metrics
            
        Returns:
            Domain adaptation score
        """
        domain_adaptations = data.get('domain_adaptations', [])
        if not domain_adaptations:
            return 0.0
        
        adaptation_scores = []
        
        for adaptation in domain_adaptations:
            if isinstance(adaptation, dict):
                adaptation = DomainAdaptation(
                    source_domain=adaptation.get('source_domain', ''),
                    target_domain=adaptation.get('target_domain', ''),
                    adaptation_speed=adaptation.get('adaptation_speed', 0.0),
                    adaptation_quality=adaptation.get('adaptation_quality', 0.0),
                    knowledge_transfer_rate=adaptation.get('knowledge_transfer_rate', 0.0),
                    adaptation_efficiency=adaptation.get('adaptation_efficiency', 0.0)
                )
            
            # Evaluate adaptation components
            speed_score = self._evaluate_adaptation_speed(adaptation)
            quality_score = self._evaluate_adaptation_quality(adaptation)
            transfer_score = self._evaluate_knowledge_transfer_rate(adaptation)
            efficiency_score = self._evaluate_adaptation_efficiency(adaptation)
            
            # Calculate weighted adaptation score
            weights = {
                'speed': 0.25,
                'quality': 0.3,
                'transfer': 0.25,
                'efficiency': 0.2
            }
            
            adaptation_score = (
                speed_score * weights['speed'] +
                quality_score * weights['quality'] +
                transfer_score * weights['transfer'] +
                efficiency_score * weights['efficiency']
            )
            
            adaptation_scores.append(adaptation_score)
        
        return statistics.mean(adaptation_scores) * 100
    
    def calculate_knowledge_transfer(self, data: Dict[str, Any]) -> float:
        """
        Calculate knowledge transfer efficiency score.
        
        Args:
            data: Data containing knowledge transfer metrics
            
        Returns:
            Knowledge transfer score
        """
        knowledge_transfers = data.get('knowledge_transfers', [])
        if not knowledge_transfers:
            return 0.0
        
        transfer_scores = []
        
        for transfer in knowledge_transfers:
            if isinstance(transfer, dict):
                transfer = KnowledgeTransfer(
                    source_domains=transfer.get('source_domains', []),
                    target_domain=transfer.get('target_domain', ''),
                    transfer_effectiveness=transfer.get('transfer_effectiveness', 0.0),
                    transfer_efficiency=transfer.get('transfer_efficiency', 0.0),
                    knowledge_retention=transfer.get('knowledge_retention', 0.0),
                    transfer_bottlenecks=transfer.get('transfer_bottlenecks', [])
                )
            
            # Evaluate transfer components
            effectiveness_score = self._evaluate_transfer_effectiveness(transfer)
            efficiency_score = self._evaluate_transfer_efficiency(transfer)
            retention_score = self._evaluate_knowledge_retention(transfer)
            bottleneck_score = self._evaluate_transfer_bottlenecks(transfer)
            
            # Calculate weighted transfer score
            weights = {
                'effectiveness': 0.35,
                'efficiency': 0.25,
                'retention': 0.25,
                'bottleneck': 0.15
            }
            
            transfer_score = (
                effectiveness_score * weights['effectiveness'] +
                efficiency_score * weights['efficiency'] +
                retention_score * weights['retention'] +
                bottleneck_score * weights['bottleneck']
            )
            
            transfer_scores.append(transfer_score)
        
        return statistics.mean(transfer_scores) * 100
    
    def calculate_generalization(self, data: Dict[str, Any]) -> float:
        """
        Calculate generalization ability assessment score.
        
        Args:
            data: Data containing generalization metrics
            
        Returns:
            Generalization score
        """
        generalization_metrics = data.get('generalization_metrics', [])
        if not generalization_metrics:
            return 0.0
        
        generalization_scores = []
        
        for metrics in generalization_metrics:
            if isinstance(metrics, dict):
                metrics = GeneralizationMetrics(
                    training_domains=metrics.get('training_domains', []),
                    test_domains=metrics.get('test_domains', []),
                    generalization_score=metrics.get('generalization_score', 0.0),
                    overfitting_measure=metrics.get('overfitting_measure', 0.0),
                    robustness_score=metrics.get('robustness_score', 0.0),
                    domain_gap_analysis=metrics.get('domain_gap_analysis', {})
                )
            
            # Evaluate generalization components
            generalization_ability = self._evaluate_generalization_ability(metrics)
            overfitting_resistance = self._evaluate_overfitting_resistance(metrics)
            robustness = self._evaluate_robustness(metrics)
            domain_gap_handling = self._evaluate_domain_gap_handling(metrics)
            
            # Calculate weighted generalization score
            weights = {
                'generalization_ability': 0.3,
                'overfitting_resistance': 0.25,
                'robustness': 0.25,
                'domain_gap_handling': 0.2
            }
            
            generalization_score = (
                generalization_ability * weights['generalization_ability'] +
                overfitting_resistance * weights['overfitting_resistance'] +
                robustness * weights['robustness'] +
                domain_gap_handling * weights['domain_gap_handling']
            )
            
            generalization_scores.append(generalization_score)
        
        return statistics.mean(generalization_scores) * 100
    
    def calculate_consistency(self, data: Dict[str, Any]) -> float:
        """
        Calculate cross-domain consistency evaluation score.
        
        Args:
            data: Data containing consistency metrics
            
        Returns:
            Consistency score
        """
        domain_performances = data.get('domain_performances', [])
        consistency_metrics = data.get('consistency_metrics', {})
        
        if not domain_performances and not consistency_metrics:
            return 0.0
        
        # Evaluate consistency components
        performance_consistency = self._evaluate_performance_consistency(domain_performances)
        behavioral_consistency = self._evaluate_behavioral_consistency(consistency_metrics)
        strategy_consistency = self._evaluate_strategy_consistency(consistency_metrics)
        quality_consistency = self._evaluate_quality_consistency(consistency_metrics)
        
        # Calculate weighted consistency score
        weights = {
            'performance_consistency': 0.3,
            'behavioral_consistency': 0.25,
            'strategy_consistency': 0.25,
            'quality_consistency': 0.2
        }
        
        consistency_score = (
            performance_consistency * weights['performance_consistency'] +
            behavioral_consistency * weights['behavioral_consistency'] +
            strategy_consistency * weights['strategy_consistency'] +
            quality_consistency * weights['quality_consistency']
        )
        
        return consistency_score * 100
    
    def _evaluate_adaptation_speed(self, adaptation: DomainAdaptation) -> float:
        """Evaluate adaptation speed."""
        adaptation_speed = adaptation.adaptation_speed
        
        # Higher adaptation speed is better
        speed_score = min(1.0, adaptation_speed / 100.0)  # Normalize to reasonable range
        
        return speed_score
    
    def _evaluate_adaptation_quality(self, adaptation: DomainAdaptation) -> float:
        """Evaluate adaptation quality."""
        adaptation_quality = adaptation.adaptation_quality
        
        # Higher adaptation quality is better
        quality_score = adaptation_quality / 100.0  # Normalize to 0-1
        
        return quality_score
    
    def _evaluate_knowledge_transfer_rate(self, adaptation: DomainAdaptation) -> float:
        """Evaluate knowledge transfer rate."""
        knowledge_transfer_rate = adaptation.knowledge_transfer_rate
        
        # Higher transfer rate is better
        transfer_score = knowledge_transfer_rate / 100.0  # Normalize to 0-1
        
        return transfer_score
    
    def _evaluate_adaptation_efficiency(self, adaptation: DomainAdaptation) -> float:
        """Evaluate adaptation efficiency."""
        adaptation_efficiency = adaptation.adaptation_efficiency
        
        # Higher efficiency is better
        efficiency_score = adaptation_efficiency / 100.0  # Normalize to 0-1
        
        return efficiency_score
    
    def _evaluate_transfer_effectiveness(self, transfer: KnowledgeTransfer) -> float:
        """Evaluate transfer effectiveness."""
        transfer_effectiveness = transfer.transfer_effectiveness
        
        # Higher effectiveness is better
        effectiveness_score = transfer_effectiveness / 100.0  # Normalize to 0-1
        
        return effectiveness_score
    
    def _evaluate_transfer_efficiency(self, transfer: KnowledgeTransfer) -> float:
        """Evaluate transfer efficiency."""
        transfer_efficiency = transfer.transfer_efficiency
        
        # Higher efficiency is better
        efficiency_score = transfer_efficiency / 100.0  # Normalize to 0-1
        
        return efficiency_score
    
    def _evaluate_knowledge_retention(self, transfer: KnowledgeTransfer) -> float:
        """Evaluate knowledge retention."""
        knowledge_retention = transfer.knowledge_retention
        
        # Higher retention is better
        retention_score = knowledge_retention / 100.0  # Normalize to 0-1
        
        return retention_score
    
    def _evaluate_transfer_bottlenecks(self, transfer: KnowledgeTransfer) -> float:
        """Evaluate transfer bottlenecks."""
        transfer_bottlenecks = transfer.transfer_bottlenecks
        source_domains = transfer.source_domains
        
        if not source_domains:
            return 1.0
        
        # Fewer bottlenecks are better
        bottleneck_ratio = len(transfer_bottlenecks) / len(source_domains)
        bottleneck_score = max(0.0, 1.0 - bottleneck_ratio)
        
        return bottleneck_score
    
    def _evaluate_generalization_ability(self, metrics: GeneralizationMetrics) -> float:
        """Evaluate generalization ability."""
        generalization_score = metrics.generalization_score
        
        # Higher generalization score is better
        generalization_ability = generalization_score / 100.0  # Normalize to 0-1
        
        return generalization_ability
    
    def _evaluate_overfitting_resistance(self, metrics: GeneralizationMetrics) -> float:
        """Evaluate overfitting resistance."""
        overfitting_measure = metrics.overfitting_measure
        
        # Lower overfitting measure is better
        overfitting_resistance = max(0.0, 1.0 - overfitting_measure)
        
        return overfitting_resistance
    
    def _evaluate_robustness(self, metrics: GeneralizationMetrics) -> float:
        """Evaluate robustness."""
        robustness_score = metrics.robustness_score
        
        # Higher robustness score is better
        robustness = robustness_score / 100.0  # Normalize to 0-1
        
        return robustness
    
    def _evaluate_domain_gap_handling(self, metrics: GeneralizationMetrics) -> float:
        """Evaluate domain gap handling."""
        domain_gap_analysis = metrics.domain_gap_analysis
        
        if not domain_gap_analysis:
            return 0.0
        
        # Calculate average domain gap handling
        gap_scores = list(domain_gap_analysis.values())
        avg_gap_handling = statistics.mean(gap_scores) if gap_scores else 0.0
        
        # Higher gap handling score is better
        gap_handling_score = avg_gap_handling / 100.0  # Normalize to 0-1
        
        return gap_handling_score
    
    def _evaluate_performance_consistency(self, domain_performances: List[Dict[str, Any]]) -> float:
        """Evaluate performance consistency across domains."""
        if not domain_performances:
            return 0.0
        
        # Extract performance scores
        performance_scores = []
        for performance in domain_performances:
            if isinstance(performance, dict):
                performance = DomainPerformance(
                    domain_name=performance.get('domain_name', ''),
                    accuracy=performance.get('accuracy', 0.0),
                    efficiency=performance.get('efficiency', 0.0),
                    adaptation_time=performance.get('adaptation_time', 0.0),
                    resource_usage=performance.get('resource_usage', 0.0),
                    domain_specific_metrics=performance.get('domain_specific_metrics', {})
                )
            
            # Calculate overall performance score
            overall_score = (performance.accuracy + performance.efficiency) / 2.0
            performance_scores.append(overall_score)
        
        if len(performance_scores) < 2:
            return 1.0
        
        # Calculate consistency (lower variance is better)
        variance = statistics.variance(performance_scores)
        consistency_score = max(0.0, 1.0 - variance * 2)  # Scale variance impact
        
        return consistency_score
    
    def _evaluate_behavioral_consistency(self, consistency_metrics: Dict[str, Any]) -> float:
        """Evaluate behavioral consistency across domains."""
        if not consistency_metrics:
            return 0.0
        
        behavioral_patterns = consistency_metrics.get('behavioral_patterns', {})
        pattern_consistency = consistency_metrics.get('pattern_consistency', 0.0)
        
        # Higher pattern consistency is better
        behavioral_consistency = pattern_consistency / 100.0  # Normalize to 0-1
        
        return behavioral_consistency
    
    def _evaluate_strategy_consistency(self, consistency_metrics: Dict[str, Any]) -> float:
        """Evaluate strategy consistency across domains."""
        if not consistency_metrics:
            return 0.0
        
        strategy_patterns = consistency_metrics.get('strategy_patterns', {})
        strategy_consistency = consistency_metrics.get('strategy_consistency', 0.0)
        
        # Higher strategy consistency is better
        strategy_score = strategy_consistency / 100.0  # Normalize to 0-1
        
        return strategy_score
    
    def _evaluate_quality_consistency(self, consistency_metrics: Dict[str, Any]) -> float:
        """Evaluate quality consistency across domains."""
        if not consistency_metrics:
            return 0.0
        
        quality_metrics = consistency_metrics.get('quality_metrics', {})
        quality_consistency = consistency_metrics.get('quality_consistency', 0.0)
        
        # Higher quality consistency is better
        quality_score = quality_consistency / 100.0  # Normalize to 0-1
        
        return quality_score
    
    def calculate_domain_similarity_matrix(self, data: Dict[str, Any]) -> Dict[str, Dict[str, float]]:
        """
        Calculate domain similarity matrix.
        
        Args:
            data: Data containing domain information
            
        Returns:
            Domain similarity matrix
        """
        domain_features = data.get('domain_features', {})
        domains = list(domain_features.keys())
        
        similarity_matrix = {}
        
        for domain1 in domains:
            similarity_matrix[domain1] = {}
            for domain2 in domains:
                if domain1 == domain2:
                    similarity_matrix[domain1][domain2] = 1.0
                else:
                    similarity = self._calculate_domain_similarity(
                        domain_features[domain1], 
                        domain_features[domain2]
                    )
                    similarity_matrix[domain1][domain2] = similarity
        
        return similarity_matrix
    
    def _calculate_domain_similarity(self, features1: Dict[str, Any], features2: Dict[str, Any]) -> float:
        """Calculate similarity between two domains based on their features."""
        if not features1 or not features2:
            return 0.0
        
        # Extract common features
        common_features = set(features1.keys()) & set(features2.keys())
        
        if not common_features:
            return 0.0
        
        # Calculate similarity based on common features
        similarity_scores = []
        
        for feature in common_features:
            value1 = features1[feature]
            value2 = features2[feature]
            
            if isinstance(value1, (int, float)) and isinstance(value2, (int, float)):
                # Normalize values to 0-1 range
                max_val = max(abs(value1), abs(value2))
                if max_val > 0:
                    norm_val1 = value1 / max_val
                    norm_val2 = value2 / max_val
                    similarity = 1.0 - abs(norm_val1 - norm_val2)
                else:
                    similarity = 1.0
            elif isinstance(value1, (list, set)) and isinstance(value2, (list, set)):
                # Calculate Jaccard similarity for sets/lists
                set1 = set(value1)
                set2 = set(value2)
                intersection = len(set1 & set2)
                union = len(set1 | set2)
                similarity = intersection / union if union > 0 else 0.0
            elif value1 == value2:
                similarity = 1.0
            else:
                similarity = 0.0
            
            similarity_scores.append(similarity)
        
        return statistics.mean(similarity_scores) if similarity_scores else 0.0
    
    def calculate_adaptation_paths(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Calculate optimal adaptation paths between domains.
        
        Args:
            data: Data containing domain adaptation information
            
        Returns:
            List of optimal adaptation paths
        """
        similarity_matrix = self.calculate_domain_similarity_matrix(data)
        domain_performances = data.get('domain_performances', [])
        
        # Build performance lookup
        performance_lookup = {}
        for performance in domain_performances:
            if isinstance(performance, dict):
                domain_name = performance.get('domain_name', '')
                accuracy = performance.get('accuracy', 0.0)
                efficiency = performance.get('efficiency', 0.0)
                performance_lookup[domain_name] = (accuracy + efficiency) / 2.0
        
        # Find optimal paths
        domains = list(similarity_matrix.keys())
        paths = []
        
        for source in domains:
            for target in domains:
                if source != target:
                    path = self._find_optimal_path(
                        source, target, similarity_matrix, performance_lookup
                    )
                    if path:
                        paths.append(path)
        
        return paths
    
    def _find_optimal_path(
        self, 
        source: str, 
        target: str, 
        similarity_matrix: Dict[str, Dict[str, float]], 
        performance_lookup: Dict[str, float]
    ) -> Dict[str, Any]:
        """Find optimal adaptation path from source to target domain."""
        # Simple pathfinding - direct adaptation
        direct_similarity = similarity_matrix[source][target]
        direct_performance = performance_lookup.get(target, 0.0)
        
        return {
            'source_domain': source,
            'target_domain': target,
            'path': [source, target],
            'similarity_score': direct_similarity,
            'expected_performance': direct_performance,
            'adaptation_difficulty': 1.0 - direct_similarity
        }
    
    def calculate_cross_domain_learning_curve(self, data: Dict[str, Any]) -> Dict[str, List[float]]:
        """
        Calculate cross-domain learning curves.
        
        Args:
            data: Data containing learning information
            
        Returns:
            Dictionary of learning curves for each domain
        """
        learning_data = data.get('learning_data', {})
        learning_curves = {}
        
        for domain, episodes in learning_data.items():
            if isinstance(episodes, list):
                # Calculate learning curve
                performance_scores = []
                for episode in episodes:
                    if isinstance(episode, dict):
                        performance = episode.get('performance', 0.0)
                        performance_scores.append(performance)
                
                learning_curves[domain] = performance_scores
        
        return learning_curves
    
    def calculate_domain_transfer_matrix(self, data: Dict[str, Any]) -> Dict[str, Dict[str, float]]:
        """
        Calculate domain transfer effectiveness matrix.
        
        Args:
            data: Data containing transfer information
            
        Returns:
            Domain transfer effectiveness matrix
        """
        transfer_data = data.get('transfer_data', {})
        domains = list(transfer_data.keys())
        
        transfer_matrix = {}
        
        for source in domains:
            transfer_matrix[source] = {}
            for target in domains:
                if source == target:
                    transfer_matrix[source][target] = 1.0
                else:
                    transfer_effectiveness = self._calculate_transfer_effectiveness(
                        transfer_data[source], 
                        transfer_data[target]
                    )
                    transfer_matrix[source][target] = transfer_effectiveness
        
        return transfer_matrix
    
    def _calculate_transfer_effectiveness(self, source_data: Dict[str, Any], target_data: Dict[str, Any]) -> float:
        """Calculate transfer effectiveness between source and target domains."""
        if not source_data or not target_data:
            return 0.0
        
        # Extract relevant metrics
        source_performance = source_data.get('performance', 0.0)
        target_performance = target_data.get('performance', 0.0)
        transfer_time = target_data.get('transfer_time', 0.0)
        knowledge_overlap = target_data.get('knowledge_overlap', 0.0)
        
        # Calculate transfer effectiveness
        if transfer_time > 0:
            effectiveness = (target_performance / source_performance) * knowledge_overlap / transfer_time
        else:
            effectiveness = 0.0
        
        return min(1.0, effectiveness)