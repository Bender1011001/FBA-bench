"""
Comparative analysis engine for FBA-Bench.

This module provides tools for comparing agent performance, including head-to-head
comparison metrics, performance ranking algorithms, strength/weakness profiling,
improvement tracking over time, benchmark standardization capabilities, normalization
methods for fair comparison, and performance gap analysis.
"""

import math
import statistics
from typing import Dict, Any, List, Optional, Tuple, Union, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import numpy as np
from scipy import stats
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

from .base import BaseMetric, MetricConfig


class ComparisonType(Enum):
    """Types of comparative analysis."""
    HEAD_TO_HEAD = "head_to_head"
    RANKING = "ranking"
    STRENGTH_WEAKNESS = "strength_weakness"
    IMPROVEMENT_TRACKING = "improvement_tracking"
    BENCHMARK_STANDARDIZATION = "benchmark_standardization"
    NORMALIZATION = "normalization"
    PERFORMANCE_GAP = "performance_gap"


class RankingMethod(Enum):
    """Methods for performance ranking."""
    MEAN_RANK = "mean_rank"
    WEIGHTED_SCORE = "weighted_score"
    BAYESIAN_RANKING = "bayesian_ranking"
    ELO_RATING = "elo_rating"
    CONDORCET_METHOD = "condorcet_method"


@dataclass
class AgentComparison:
    """Agent comparison data."""
    agent1_name: str
    agent2_name: str
    comparison_metrics: Dict[str, float]
    overall_winner: str
    win_margin: float
    statistical_significance: float
    confidence_interval: Tuple[float, float]


@dataclass
class PerformanceRanking:
    """Performance ranking data."""
    agent_name: str
    rank: int
    score: float
    confidence: float
    rank_stability: float
    comparison_group: str


@dataclass
class StrengthWeaknessProfile:
    """Strength and weakness profile data."""
    agent_name: str
    strengths: List[str]
    weaknesses: List[str]
    strength_scores: Dict[str, float]
    weakness_scores: Dict[str, float]
    overall_balance: float
    improvement_potential: float


@dataclass
class ImprovementTracking:
    """Improvement tracking data."""
    agent_name: str
    time_points: List[datetime]
    performance_scores: List[float]
    improvement_rate: float
    improvement_trend: str
    predicted_performance: float
    key_improvement_areas: List[str]


@dataclass
class BenchmarkStandard:
    """Benchmark standard data."""
    standard_name: str
    standard_metrics: Dict[str, Tuple[float, float]]  # (min, max) ranges
    normalization_method: str
    weighting_scheme: Dict[str, float]
    validation_criteria: List[str]


@dataclass
class NormalizationResult:
    """Normalization result data."""
    original_values: Dict[str, float]
    normalized_values: Dict[str, float]
    normalization_method: str
    scaling_factors: Dict[str, float]
    quality_score: float


@dataclass
class PerformanceGap:
    """Performance gap analysis data."""
    agent_name: str
    benchmark_name: str
    gap_metrics: Dict[str, float]
    overall_gap: float
    gap_significance: float
    gap_trend: str
    closure_recommendations: List[str]


class ComparativeAnalysisEngine(BaseMetric):
    """
    Advanced comparative analysis engine for agent performance.
    
    This class provides comprehensive tools for comparing agent performance,
    including head-to-head comparisons, ranking algorithms, strength/weakness
    profiling, improvement tracking, benchmark standardization, normalization
    methods, and performance gap analysis.
    """
    
    def __init__(self, config: MetricConfig = None):
        """
        Initialize comparative analysis engine.
        
        Args:
            config: Metric configuration (uses defaults if None)
        """
        if config is None:
            config = MetricConfig(
                name="comparative_analysis",
                description="Comparative analysis engine",
                unit="score",
                min_value=0.0,
                max_value=100.0,
                target_value=85.0
            )
        
        super().__init__(config)
        
        # Sub-metric configurations
        self.head_to_head_config = MetricConfig(
            name="head_to_head",
            description="Head-to-head comparison metrics",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.ranking_config = MetricConfig(
            name="ranking",
            description="Performance ranking algorithms",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.strength_weakness_config = MetricConfig(
            name="strength_weakness",
            description="Strength/weakness profiling",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.improvement_tracking_config = MetricConfig(
            name="improvement_tracking",
            description="Improvement tracking over time",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.benchmark_standardization_config = MetricConfig(
            name="benchmark_standardization",
            description="Benchmark standardization capabilities",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.normalization_config = MetricConfig(
            name="normalization",
            description="Normalization methods for fair comparison",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
        
        self.performance_gap_config = MetricConfig(
            name="performance_gap",
            description="Performance gap analysis",
            unit="score",
            min_value=0.0,
            max_value=100.0
        )
    
    def calculate(self, data: Dict[str, Any]) -> float:
        """
        Calculate comparative analysis score.
        
        Args:
            data: Data containing comparative analysis metrics
            
        Returns:
            Overall comparative analysis score
        """
        # Calculate sub-metric scores
        head_to_head = self.calculate_head_to_head_score(data)
        ranking = self.calculate_ranking_score(data)
        strength_weakness = self.calculate_strength_weakness_score(data)
        improvement_tracking = self.calculate_improvement_tracking_score(data)
        benchmark_standardization = self.calculate_benchmark_standardization_score(data)
        normalization = self.calculate_normalization_score(data)
        performance_gap = self.calculate_performance_gap_score(data)
        
        # Calculate weighted average
        weights = {
            'head_to_head': 0.15,
            'ranking': 0.15,
            'strength_weakness': 0.15,
            'improvement_tracking': 0.15,
            'benchmark_standardization': 0.13,
            'normalization': 0.13,
            'performance_gap': 0.14
        }
        
        overall_score = (
            head_to_head * weights['head_to_head'] +
            ranking * weights['ranking'] +
            strength_weakness * weights['strength_weakness'] +
            improvement_tracking * weights['improvement_tracking'] +
            benchmark_standardization * weights['benchmark_standardization'] +
            normalization * weights['normalization'] +
            performance_gap * weights['performance_gap']
        )
        
        return overall_score
    
    def calculate_head_to_head_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate head-to-head comparison score.
        
        Args:
            data: Data containing head-to-head comparison metrics
            
        Returns:
            Head-to-head comparison score
        """
        comparisons = data.get('head_to_head_comparisons', [])
        if not comparisons:
            return 0.0
        
        comparison_scores = []
        
        for comparison in comparisons:
            # Evaluate comparison components
            comparison_quality = comparison.get('comparison_quality', 0.0)
            statistical_significance = comparison.get('statistical_significance', 0.0)
            metric_coverage = comparison.get('metric_coverage', 0.0)
            
            # Calculate weighted comparison score
            weights = {
                'comparison_quality': 0.4,
                'statistical_significance': 0.3,
                'metric_coverage': 0.3
            }
            
            comparison_score = (
                comparison_quality * weights['comparison_quality'] +
                statistical_significance * weights['statistical_significance'] +
                metric_coverage * weights['metric_coverage']
            )
            
            comparison_scores.append(comparison_score)
        
        return statistics.mean(comparison_scores) * 100
    
    def calculate_ranking_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate ranking score.
        
        Args:
            data: Data containing ranking metrics
            
        Returns:
            Ranking score
        """
        rankings = data.get('rankings', [])
        if not rankings:
            return 0.0
        
        ranking_scores = []
        
        for ranking in rankings:
            # Evaluate ranking components
            ranking_accuracy = ranking.get('ranking_accuracy', 0.0)
            rank_stability = ranking.get('rank_stability', 0.0)
            method_diversity = ranking.get('method_diversity', 0.0)
            
            # Calculate weighted ranking score
            weights = {
                'ranking_accuracy': 0.4,
                'rank_stability': 0.3,
                'method_diversity': 0.3
            }
            
            ranking_score = (
                ranking_accuracy * weights['ranking_accuracy'] +
                rank_stability * weights['rank_stability'] +
                method_diversity * weights['method_diversity']
            )
            
            ranking_scores.append(ranking_score)
        
        return statistics.mean(ranking_scores) * 100
    
    def calculate_strength_weakness_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate strength/weakness profiling score.
        
        Args:
            data: Data containing strength/weakness metrics
            
        Returns:
            Strength/weakness profiling score
        """
        profiles = data.get('strength_weakness_profiles', [])
        if not profiles:
            return 0.0
        
        profile_scores = []
        
        for profile in profiles:
            # Evaluate profile components
            strength_identification = profile.get('strength_identification', 0.0)
            weakness_identification = profile.get('weakness_identification', 0.0)
            balance_assessment = profile.get('balance_assessment', 0.0)
            
            # Calculate weighted profile score
            weights = {
                'strength_identification': 0.35,
                'weakness_identification': 0.35,
                'balance_assessment': 0.3
            }
            
            profile_score = (
                strength_identification * weights['strength_identification'] +
                weakness_identification * weights['weakness_identification'] +
                balance_assessment * weights['balance_assessment']
            )
            
            profile_scores.append(profile_score)
        
        return statistics.mean(profile_scores) * 100
    
    def calculate_improvement_tracking_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate improvement tracking score.
        
        Args:
            data: Data containing improvement tracking metrics
            
        Returns:
            Improvement tracking score
        """
        tracking_data = data.get('improvement_tracking', [])
        if not tracking_data:
            return 0.0
        
        tracking_scores = []
        
        for tracking in tracking_data:
            # Evaluate tracking components
            trend_accuracy = tracking.get('trend_accuracy', 0.0)
            prediction_accuracy = tracking.get('prediction_accuracy', 0.0)
            improvement_detection = tracking.get('improvement_detection', 0.0)
            
            # Calculate weighted tracking score
            weights = {
                'trend_accuracy': 0.35,
                'prediction_accuracy': 0.35,
                'improvement_detection': 0.3
            }
            
            tracking_score = (
                trend_accuracy * weights['trend_accuracy'] +
                prediction_accuracy * weights['prediction_accuracy'] +
                improvement_detection * weights['improvement_detection']
            )
            
            tracking_scores.append(tracking_score)
        
        return statistics.mean(tracking_scores) * 100
    
    def calculate_benchmark_standardization_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate benchmark standardization score.
        
        Args:
            data: Data containing benchmark standardization metrics
            
        Returns:
            Benchmark standardization score
        """
        standards = data.get('benchmark_standards', [])
        if not standards:
            return 0.0
        
        standard_scores = []
        
        for standard in standards:
            # Evaluate standard components
            standard_completeness = standard.get('standard_completeness', 0.0)
            validation_effectiveness = standard.get('validation_effectiveness', 0.0)
            adoption_rate = standard.get('adoption_rate', 0.0)
            
            # Calculate weighted standard score
            weights = {
                'standard_completeness': 0.4,
                'validation_effectiveness': 0.3,
                'adoption_rate': 0.3
            }
            
            standard_score = (
                standard_completeness * weights['standard_completeness'] +
                validation_effectiveness * weights['validation_effectiveness'] +
                adoption_rate * weights['adoption_rate']
            )
            
            standard_scores.append(standard_score)
        
        return statistics.mean(standard_scores) * 100
    
    def calculate_normalization_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate normalization score.
        
        Args:
            data: Data containing normalization metrics
            
        Returns:
            Normalization score
        """
        normalizations = data.get('normalizations', [])
        if not normalizations:
            return 0.0
        
        normalization_scores = []
        
        for normalization in normalizations:
            # Evaluate normalization components
            normalization_effectiveness = normalization.get('normalization_effectiveness', 0.0)
            fairness_improvement = normalization.get('fairness_improvement', 0.0)
            method_appropriateness = normalization.get('method_appropriateness', 0.0)
            
            # Calculate weighted normalization score
            weights = {
                'normalization_effectiveness': 0.4,
                'fairness_improvement': 0.3,
                'method_appropriateness': 0.3
            }
            
            normalization_score = (
                normalization_effectiveness * weights['normalization_effectiveness'] +
                fairness_improvement * weights['fairness_improvement'] +
                method_appropriateness * weights['method_appropriateness']
            )
            
            normalization_scores.append(normalization_score)
        
        return statistics.mean(normalization_scores) * 100
    
    def calculate_performance_gap_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate performance gap analysis score.
        
        Args:
            data: Data containing performance gap metrics
            
        Returns:
            Performance gap analysis score
        """
        gaps = data.get('performance_gaps', [])
        if not gaps:
            return 0.0
        
        gap_scores = []
        
        for gap in gaps:
            # Evaluate gap components
            gap_identification = gap.get('gap_identification', 0.0)
            gap_significance = gap.get('gap_significance', 0.0)
            recommendation_quality = gap.get('recommendation_quality', 0.0)
            
            # Calculate weighted gap score
            weights = {
                'gap_identification': 0.35,
                'gap_significance': 0.3,
                'recommendation_quality': 0.35
            }
            
            gap_score = (
                gap_identification * weights['gap_identification'] +
                gap_significance * weights['gap_significance'] +
                recommendation_quality * weights['recommendation_quality']
            )
            
            gap_scores.append(gap_score)
        
        return statistics.mean(gap_scores) * 100
    
    def perform_head_to_head_comparison(
        self,
        agent1_data: Dict[str, Any],
        agent2_data: Dict[str, Any],
        metrics: List[str],
        weights: Dict[str, float] = None
    ) -> AgentComparison:
        """
        Perform head-to-head comparison between two agents.
        
        Args:
            agent1_data: Performance data for agent 1
            agent2_data: Performance data for agent 2
            metrics: List of metrics to compare
            weights: Optional weights for metrics
            
        Returns:
            Agent comparison result
        """
        if weights is None:
            weights = {metric: 1.0 / len(metrics) for metric in metrics}
        
        # Calculate comparison metrics
        comparison_metrics = {}
        agent1_scores = []
        agent2_scores = []
        
        for metric in metrics:
            agent1_value = agent1_data.get(metric, 0.0)
            agent2_value = agent2_data.get(metric, 0.0)
            
            # Calculate normalized difference
            if agent1_value + agent2_value > 0:
                diff = (agent1_value - agent2_value) / max(agent1_value, agent2_value)
            else:
                diff = 0.0
            
            comparison_metrics[metric] = diff
            agent1_scores.append(agent1_value)
            agent2_scores.append(agent2_value)
        
        # Calculate overall scores
        agent1_overall = sum(agent1_scores[i] * weights[metrics[i]] for i in range(len(metrics)))
        agent2_overall = sum(agent2_scores[i] * weights[metrics[i]] for i in range(len(metrics)))
        
        # Determine winner
        if agent1_overall > agent2_overall:
            overall_winner = agent1_data.get('name', 'Agent 1')
            win_margin = (agent1_overall - agent2_overall) / max(agent1_overall, agent2_overall)
        elif agent2_overall > agent1_overall:
            overall_winner = agent2_data.get('name', 'Agent 2')
            win_margin = (agent2_overall - agent1_overall) / max(agent1_overall, agent2_overall)
        else:
            overall_winner = "Tie"
            win_margin = 0.0
        
        # Calculate statistical significance
        if len(agent1_scores) > 1 and len(agent2_scores) > 1:
            try:
                statistic, p_value = stats.ttest_ind(agent1_scores, agent2_scores)
                statistical_significance = 1.0 - p_value
            except:
                statistical_significance = 0.0
        else:
            statistical_significance = 0.0
        
        # Calculate confidence interval for win margin
        if len(agent1_scores) > 1 and len(agent2_scores) > 1:
            std_err = statistics.stdev([s1 - s2 for s1, s2 in zip(agent1_scores, agent2_scores)]) / math.sqrt(len(agent1_scores))
            margin_of_error = 1.96 * std_err  # 95% CI
            confidence_interval = (win_margin - margin_of_error, win_margin + margin_of_error)
        else:
            confidence_interval = (0.0, 0.0)
        
        return AgentComparison(
            agent1_name=agent1_data.get('name', 'Agent 1'),
            agent2_name=agent2_data.get('name', 'Agent 2'),
            comparison_metrics=comparison_metrics,
            overall_winner=overall_winner,
            win_margin=win_margin,
            statistical_significance=statistical_significance,
            confidence_interval=confidence_interval
        )
    
    def rank_agents(
        self,
        agents_data: List[Dict[str, Any]],
        metrics: List[str],
        method: RankingMethod = RankingMethod.WEIGHTED_SCORE,
        weights: Dict[str, float] = None
    ) -> List[PerformanceRanking]:
        """
        Rank agents based on performance metrics.
        
        Args:
            agents_data: List of agent performance data
            metrics: List of metrics to use for ranking
            method: Ranking method to use
            weights: Optional weights for metrics
            
        Returns:
            List of performance rankings
        """
        if weights is None:
            weights = {metric: 1.0 / len(metrics) for metric in metrics}
        
        if method == RankingMethod.WEIGHTED_SCORE:
            return self._rank_by_weighted_score(agents_data, metrics, weights)
        elif method == RankingMethod.MEAN_RANK:
            return self._rank_by_mean_rank(agents_data, metrics, weights)
        elif method == RankingMethod.BAYESIAN_RANKING:
            return self._rank_by_bayesian(agents_data, metrics, weights)
        elif method == RankingMethod.ELO_RATING:
            return self._rank_by_elo(agents_data, metrics, weights)
        elif method == RankingMethod.CONDORCET_METHOD:
            return self._rank_by_condorcet(agents_data, metrics, weights)
        else:
            raise ValueError(f"Unsupported ranking method: {method}")
    
    def _rank_by_weighted_score(
        self,
        agents_data: List[Dict[str, Any]],
        metrics: List[str],
        weights: Dict[str, float]
    ) -> List[PerformanceRanking]:
        """Rank agents by weighted score."""
        rankings = []
        
        for agent_data in agents_data:
            # Calculate weighted score
            score = sum(agent_data.get(metric, 0.0) * weights[metric] for metric in metrics)
            
            # Calculate confidence (simplified)
            metric_values = [agent_data.get(metric, 0.0) for metric in metrics]
            confidence = 1.0 - (statistics.stdev(metric_values) / statistics.mean(metric_values) if statistics.mean(metric_values) > 0 else 0.0)
            confidence = max(0.0, min(1.0, confidence))
            
            # Calculate rank stability (simplified)
            rank_stability = confidence  # Simplified for now
            
            rankings.append(PerformanceRanking(
                agent_name=agent_data.get('name', 'Unknown'),
                rank=0,  # Will be assigned later
                score=score,
                confidence=confidence,
                rank_stability=rank_stability,
                comparison_group="all_agents"
            ))
        
        # Sort by score and assign ranks
        rankings.sort(key=lambda x: x.score, reverse=True)
        for i, ranking in enumerate(rankings):
            ranking.rank = i + 1
        
        return rankings
    
    def _rank_by_mean_rank(
        self,
        agents_data: List[Dict[str, Any]],
        metrics: List[str],
        weights: Dict[str, float]
    ) -> List[PerformanceRanking]:
        """Rank agents by mean rank across metrics."""
        rankings = []
        
        # Calculate ranks for each metric
        metric_ranks = {}
        for metric in metrics:
            values = [(i, agent_data.get(metric, 0.0)) for i, agent_data in enumerate(agents_data)]
            values.sort(key=lambda x: x[1], reverse=True)
            metric_ranks[metric] = {i: rank + 1 for rank, (i, _) in enumerate(values)}
        
        # Calculate mean ranks
        for i, agent_data in enumerate(agents_data):
            mean_rank = statistics.mean([metric_ranks[metric][i] * weights[metric] for metric in metrics])
            
            # Calculate confidence and stability
            rank_values = [metric_ranks[metric][i] for metric in metrics]
            confidence = 1.0 - (statistics.stdev(rank_values) / statistics.mean(rank_values) if statistics.mean(rank_values) > 0 else 0.0)
            confidence = max(0.0, min(1.0, confidence))
            rank_stability = confidence
            
            rankings.append(PerformanceRanking(
                agent_name=agent_data.get('name', 'Unknown'),
                rank=0,  # Will be assigned later
                score=1.0 / mean_rank,  # Convert to score (lower rank = higher score)
                confidence=confidence,
                rank_stability=rank_stability,
                comparison_group="all_agents"
            ))
        
        # Sort by score and assign ranks
        rankings.sort(key=lambda x: x.score, reverse=True)
        for i, ranking in enumerate(rankings):
            ranking.rank = i + 1
        
        return rankings
    
    def _rank_by_bayesian(
        self,
        agents_data: List[Dict[str, Any]],
        metrics: List[str],
        weights: Dict[str, float]
    ) -> List[PerformanceRanking]:
        """Rank agents using Bayesian ranking."""
        rankings = []
        
        # Calculate Bayesian scores with uncertainty
        for agent_data in agents_data:
            metric_values = [agent_data.get(metric, 0.0) for metric in metrics]
            
            # Calculate weighted mean
            weighted_mean = sum(val * weights[metric] for val, metric in zip(metric_values, metrics))
            
            # Calculate uncertainty (standard error)
            variance = sum(weights[metric]**2 * (val - weighted_mean)**2 for val, metric in zip(metric_values, metrics))
            uncertainty = math.sqrt(variance)
            
            # Bayesian score (mean - k * uncertainty)
            k = 1.0  # Uncertainty penalty factor
            bayesian_score = weighted_mean - k * uncertainty
            
            # Confidence based on uncertainty
            confidence = 1.0 / (1.0 + uncertainty)
            
            # Rank stability
            rank_stability = confidence
            
            rankings.append(PerformanceRanking(
                agent_name=agent_data.get('name', 'Unknown'),
                rank=0,  # Will be assigned later
                score=bayesian_score,
                confidence=confidence,
                rank_stability=rank_stability,
                comparison_group="all_agents"
            ))
        
        # Sort by score and assign ranks
        rankings.sort(key=lambda x: x.score, reverse=True)
        for i, ranking in enumerate(rankings):
            ranking.rank = i + 1
        
        return rankings
    
    def _rank_by_elo(
        self,
        agents_data: List[Dict[str, Any]],
        metrics: List[str],
        weights: Dict[str, float]
    ) -> List[PerformanceRanking]:
        """Rank agents using ELO rating system."""
        rankings = []
        
        # Initialize ELO ratings
        elo_ratings = {i: 1500 for i in range(len(agents_data))}
        
        # Compare all pairs of agents
        for i in range(len(agents_data)):
            for j in range(i + 1, len(agents_data)):
                agent1_score = sum(agents_data[i].get(metric, 0.0) * weights[metric] for metric in metrics)
                agent2_score = sum(agents_data[j].get(metric, 0.0) * weights[metric] for metric in metrics)
                
                # Determine winner
                if agent1_score > agent2_score:
                    winner, loser = i, j
                elif agent2_score > agent1_score:
                    winner, loser = j, i
                else:
                    continue  # Skip ties
                
                # Update ELO ratings
                k_factor = 32
                expected_winner = 1.0 / (1.0 + 10**((elo_ratings[loser] - elo_ratings[winner]) / 400))
                expected_loser = 1.0 - expected_winner
                
                elo_ratings[winner] += k_factor * (1 - expected_winner)
                elo_ratings[loser] += k_factor * (0 - expected_loser)
        
        # Create rankings based on ELO ratings
        for i, agent_data in enumerate(agents_data):
            elo_score = elo_ratings[i]
            
            # Calculate confidence based on number of comparisons
            confidence = min(1.0, len(agents_data) / 10.0)
            
            # Rank stability
            rank_stability = confidence
            
            rankings.append(PerformanceRanking(
                agent_name=agent_data.get('name', 'Unknown'),
                rank=0,  # Will be assigned later
                score=elo_score,
                confidence=confidence,
                rank_stability=rank_stability,
                comparison_group="all_agents"
            ))
        
        # Sort by score and assign ranks
        rankings.sort(key=lambda x: x.score, reverse=True)
        for i, ranking in enumerate(rankings):
            ranking.rank = i + 1
        
        return rankings
    
    def _rank_by_condorcet(
        self,
        agents_data: List[Dict[str, Any]],
        metrics: List[str],
        weights: Dict[str, float]
    ) -> List[PerformanceRanking]:
        """Rank agents using Condorcet method."""
        rankings = []
        
        # Create pairwise comparison matrix
        n_agents = len(agents_data)
        comparison_matrix = [[0] * n_agents for _ in range(n_agents)]
        
        # Fill comparison matrix
        for i in range(n_agents):
            for j in range(n_agents):
                if i != j:
                    agent1_score = sum(agents_data[i].get(metric, 0.0) * weights[metric] for metric in metrics)
                    agent2_score = sum(agents_data[j].get(metric, 0.0) * weights[metric] for metric in metrics)
                    
                    if agent1_score > agent2_score:
                        comparison_matrix[i][j] = 1
                    elif agent2_score > agent1_score:
                        comparison_matrix[i][j] = -1
        
        # Calculate Condorcet scores
        condorcet_scores = []
        for i in range(n_agents):
            wins = sum(1 for j in range(n_agents) if comparison_matrix[i][j] == 1)
            losses = sum(1 for j in range(n_agents) if comparison_matrix[i][j] == -1)
            condorcet_score = wins - losses
            condorcet_scores.append(condorcet_score)
        
        # Create rankings
        for i, agent_data in enumerate(agents_data):
            score = condorcet_scores[i]
            
            # Calculate confidence
            total_comparisons = n_agents - 1
            confidence = abs(score) / total_comparisons if total_comparisons > 0 else 0.0
            
            # Rank stability
            rank_stability = confidence
            
            rankings.append(PerformanceRanking(
                agent_name=agent_data.get('name', 'Unknown'),
                rank=0,  # Will be assigned later
                score=score,
                confidence=confidence,
                rank_stability=rank_stability,
                comparison_group="all_agents"
            ))
        
        # Sort by score and assign ranks
        rankings.sort(key=lambda x: x.score, reverse=True)
        for i, ranking in enumerate(rankings):
            ranking.rank = i + 1
        
        return rankings
    
    def create_strength_weakness_profile(
        self,
        agent_data: Dict[str, Any],
        benchmark_data: Dict[str, Any],
        metrics: List[str]
    ) -> StrengthWeaknessProfile:
        """
        Create strength and weakness profile for an agent.
        
        Args:
            agent_data: Agent performance data
            benchmark_data: Benchmark performance data
            metrics: List of metrics to analyze
            
        Returns:
            Strength and weakness profile
        """
        strengths = []
        weaknesses = []
        strength_scores = {}
        weakness_scores = {}
        
        # Identify strengths and weaknesses
        for metric in metrics:
            agent_value = agent_data.get(metric, 0.0)
            benchmark_value = benchmark_data.get(metric, 0.0)
            
            if benchmark_value > 0:
                relative_performance = agent_value / benchmark_value
            else:
                relative_performance = 1.0
            
            if relative_performance > 1.2:  # 20% above benchmark
                strengths.append(metric)
                strength_scores[metric] = relative_performance
            elif relative_performance < 0.8:  # 20% below benchmark
                weaknesses.append(metric)
                weakness_scores[metric] = relative_performance
        
        # Calculate overall balance
        if len(metrics) > 0:
            balance_ratio = len(strengths) / len(metrics)
            overall_balance = 1.0 - abs(0.5 - balance_ratio) * 2  # 1.0 = perfectly balanced
        else:
            overall_balance = 0.0
        
        # Calculate improvement potential
        if weaknesses:
            avg_weakness = statistics.mean(weakness_scores.values())
            improvement_potential = 1.0 - avg_weakness
        else:
            improvement_potential = 0.0
        
        return StrengthWeaknessProfile(
            agent_name=agent_data.get('name', 'Unknown'),
            strengths=strengths,
            weaknesses=weaknesses,
            strength_scores=strength_scores,
            weakness_scores=weakness_scores,
            overall_balance=overall_balance,
            improvement_potential=improvement_potential
        )
    
    def track_improvement(
        self,
        agent_name: str,
        historical_data: List[Dict[str, Any]],
        metrics: List[str],
        weights: Dict[str, float] = None
    ) -> ImprovementTracking:
        """
        Track improvement over time for an agent.
        
        Args:
            agent_name: Name of the agent
            historical_data: Historical performance data
            metrics: List of metrics to track
            weights: Optional weights for metrics
            
        Returns:
            Improvement tracking data
        """
        if weights is None:
            weights = {metric: 1.0 / len(metrics) for metric in metrics}
        
        # Extract time points and performance scores
        time_points = []
        performance_scores = []
        
        for data_point in historical_data:
            timestamp = data_point.get('timestamp', datetime.now())
            score = sum(data_point.get(metric, 0.0) * weights[metric] for metric in metrics)
            
            time_points.append(timestamp)
            performance_scores.append(score)
        
        # Calculate improvement rate
        if len(performance_scores) > 1:
            # Simple linear regression to find trend
            x = np.array(range(len(performance_scores))).reshape(-1, 1)
            y = np.array(performance_scores)
            
            from sklearn.linear_model import LinearRegression
            model = LinearRegression()
            model.fit(x, y)
            
            improvement_rate = model.coef_[0]
            
            # Determine trend
            if improvement_rate > 0.01:
                improvement_trend = "improving"
            elif improvement_rate < -0.01:
                improvement_trend = "declining"
            else:
                improvement_trend = "stable"
            
            # Predict future performance
            if len(performance_scores) > 0:
                predicted_performance = model.predict([[len(performance_scores)]])[0]
            else:
                predicted_performance = 0.0
        else:
            improvement_rate = 0.0
            improvement_trend = "insufficient_data"
            predicted_performance = performance_scores[0] if performance_scores else 0.0
        
        # Identify key improvement areas
        key_improvement_areas = []
        if len(historical_data) > 1:
            latest_data = historical_data[-1]
            earliest_data = historical_data[0]
            
            for metric in metrics:
                latest_value = latest_data.get(metric, 0.0)
                earliest_value = earliest_data.get(metric, 0.0)
                
                if latest_value < earliest_value:
                    key_improvement_areas.append(metric)
        
        return ImprovementTracking(
            agent_name=agent_name,
            time_points=time_points,
            performance_scores=performance_scores,
            improvement_rate=improvement_rate,
            improvement_trend=improvement_trend,
            predicted_performance=predicted_performance,
            key_improvement_areas=key_improvement_areas
        )
    
    def create_benchmark_standard(
        self,
        standard_name: str,
        reference_data: List[Dict[str, Any]],
        metrics: List[str],
        normalization_method: str = "min_max"
    ) -> BenchmarkStandard:
        """
        Create benchmark standard for comparison.
        
        Args:
            standard_name: Name of the standard
            reference_data: Reference performance data
            metrics: List of metrics to include
            normalization_method: Method for normalization
            
        Returns:
            Benchmark standard
        """
        # Calculate standard metric ranges
        standard_metrics = {}
        
        for metric in metrics:
            values = [data.get(metric, 0.0) for data in reference_data]
            if values:
                min_val = min(values)
                max_val = max(values)
                standard_metrics[metric] = (min_val, max_val)
            else:
                standard_metrics[metric] = (0.0, 100.0)  # Default range
        
        # Create weighting scheme (equal weights by default)
        weighting_scheme = {metric: 1.0 / len(metrics) for metric in metrics}
        
        # Define validation criteria
        validation_criteria = [
            "metric_range_validity",
            "data_sufficiency",
            "outlier_detection",
            "statistical_significance"
        ]
        
        return BenchmarkStandard(
            standard_name=standard_name,
            standard_metrics=standard_metrics,
            normalization_method=normalization_method,
            weighting_scheme=weighting_scheme,
            validation_criteria=validation_criteria
        )
    
    def normalize_metrics(
        self,
        agent_data: Dict[str, Any],
        benchmark_standard: BenchmarkStandard,
        method: str = "min_max"
    ) -> NormalizationResult:
        """
        Normalize metrics for fair comparison.
        
        Args:
            agent_data: Agent performance data
            benchmark_standard: Benchmark standard to use
            method: Normalization method
            
        Returns:
            Normalization result
        """
        original_values = {}
        normalized_values = {}
        scaling_factors = {}
        
        standard_metrics = benchmark_standard.standard_metrics
        
        for metric, (min_val, max_val) in standard_metrics.items():
            original_value = agent_data.get(metric, 0.0)
            original_values[metric] = original_value
            
            if method == "min_max":
                if max_val > min_val:
                    normalized_value = (original_value - min_val) / (max_val - min_val)
                else:
                    normalized_value = 0.5  # Default for zero range
                scaling_factor = 1.0 / (max_val - min_val) if max_val > min_val else 1.0
            elif method == "z_score":
                # Use mean and std dev from standard
                mean_val = (min_val + max_val) / 2.0
                std_val = (max_val - min_val) / 4.0  # Approximate std dev
                if std_val > 0:
                    normalized_value = (original_value - mean_val) / std_val
                else:
                    normalized_value = 0.0
                scaling_factor = 1.0 / std_val if std_val > 0 else 1.0
            else:
                raise ValueError(f"Unsupported normalization method: {method}")
            
            normalized_values[metric] = max(0.0, min(1.0, normalized_value))
            scaling_factors[metric] = scaling_factor
        
        # Calculate quality score
        if normalized_values:
            quality_score = statistics.mean(normalized_values.values())
        else:
            quality_score = 0.0
        
        return NormalizationResult(
            original_values=original_values,
            normalized_values=normalized_values,
            normalization_method=method,
            scaling_factors=scaling_factors,
            quality_score=quality_score
        )
    
    def analyze_performance_gap(
        self,
        agent_data: Dict[str, Any],
        benchmark_data: Dict[str, Any],
        metrics: List[str],
        weights: Dict[str, float] = None
    ) -> PerformanceGap:
        """
        Analyze performance gap between agent and benchmark.
        
        Args:
            agent_data: Agent performance data
            benchmark_data: Benchmark performance data
            metrics: List of metrics to analyze
            weights: Optional weights for metrics
            
        Returns:
            Performance gap analysis
        """
        if weights is None:
            weights = {metric: 1.0 / len(metrics) for metric in metrics}
        
        gap_metrics = {}
        gap_values = []
        
        for metric in metrics:
            agent_value = agent_data.get(metric, 0.0)
            benchmark_value = benchmark_data.get(metric, 0.0)
            
            if benchmark_value > 0:
                gap = (benchmark_value - agent_value) / benchmark_value
            else:
                gap = 0.0
            
            gap_metrics[metric] = gap
            gap_values.append(gap)
        
        # Calculate overall gap
        overall_gap = sum(gap * weights[metric] for gap, metric in zip(gap_values, metrics))
        
        # Calculate gap significance
        if len(gap_values) > 1:
            gap_std = statistics.stdev(gap_values)
            gap_significance = abs(overall_gap) / (gap_std + 1e-6)  # Avoid division by zero
        else:
            gap_significance = abs(overall_gap)
        
        # Determine gap trend
        if overall_gap > 0.1:
            gap_trend = "significant_gap"
        elif overall_gap > 0.05:
            gap_trend = "moderate_gap"
        elif overall_gap > 0.01:
            gap_trend = "minor_gap"
        else:
            gap_trend = "negligible_gap"
        
        # Generate closure recommendations
        closure_recommendations = []
        for metric, gap in gap_metrics.items():
            if gap > 0.1:  # Significant gap
                closure_recommendations.append(f"Focus on improving {metric} performance")
            elif gap > 0.05:  # Moderate gap
                closure_recommendations.append(f"Consider optimizing {metric} performance")
        
        return PerformanceGap(
            agent_name=agent_data.get('name', 'Unknown'),
            benchmark_name=benchmark_data.get('name', 'Benchmark'),
            gap_metrics=gap_metrics,
            overall_gap=overall_gap,
            gap_significance=gap_significance,
            gap_trend=gap_trend,
            closure_recommendations=closure_recommendations
        )
    
    def create_comparison_matrix(
        self,
        agents_data: List[Dict[str, Any]],
        metrics: List[str]
    ) -> Dict[str, Dict[str, float]]:
        """
        Create comparison matrix for multiple agents.
        
        Args:
            agents_data: List of agent performance data
            metrics: List of metrics to compare
            
        Returns:
            Comparison matrix
        """
        comparison_matrix = {}
        
        for i, agent1 in enumerate(agents_data):
            agent1_name = agent1.get('name', f'Agent_{i}')
            comparison_matrix[agent1_name] = {}
            
            for j, agent2 in enumerate(agents_data):
                agent2_name = agent2.get('name', f'Agent_{j}')
                
                if i == j:
                    comparison_matrix[agent1_name][agent2_name] = 1.0  # Self-comparison
                else:
                    # Calculate similarity score
                    similarity = self._calculate_agent_similarity(agent1, agent2, metrics)
                    comparison_matrix[agent1_name][agent2_name] = similarity
        
        return comparison_matrix
    
    def _calculate_agent_similarity(
        self,
        agent1_data: Dict[str, Any],
        agent2_data: Dict[str, Any],
        metrics: List[str]
    ) -> float:
        """Calculate similarity between two agents."""
        values1 = [agent1_data.get(metric, 0.0) for metric in metrics]
        values2 = [agent2_data.get(metric, 0.0) for metric in metrics]
        
        # Calculate cosine similarity
        dot_product = sum(v1 * v2 for v1, v2 in zip(values1, values2))
        magnitude1 = math.sqrt(sum(v1**2 for v1 in values1))
        magnitude2 = math.sqrt(sum(v2**2 for v2 in values2))
        
        if magnitude1 > 0 and magnitude2 > 0:
            similarity = dot_product / (magnitude1 * magnitude2)
        else:
            similarity = 0.0
        
        return similarity
    
    def cluster_agents(
        self,
        agents_data: List[Dict[str, Any]],
        metrics: List[str],
        n_clusters: int = 3
    ) -> Dict[int, List[str]]:
        """
        Cluster agents based on performance similarity.
        
        Args:
            agents_data: List of agent performance data
            metrics: List of metrics to use for clustering
            n_clusters: Number of clusters to create
            
        Returns:
            Dictionary mapping cluster IDs to agent names
        """
        # Prepare data for clustering
        feature_matrix = []
        agent_names = []
        
        for agent_data in agents_data:
            features = [agent_data.get(metric, 0.0) for metric in metrics]
            feature_matrix.append(features)
            agent_names.append(agent_data.get('name', 'Unknown'))
        
        # Perform clustering
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(feature_matrix)
        
        # Group agents by cluster
        clusters = {}
        for i, label in enumerate(cluster_labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(agent_names[i])
        
        return clusters
    
    def perform_pca_analysis(
        self,
        agents_data: List[Dict[str, Any]],
        metrics: List[str],
        n_components: int = 2
    ) -> Dict[str, Any]:
        """
        Perform PCA analysis on agent performance data.
        
        Args:
            agents_data: List of agent performance data
            metrics: List of metrics to analyze
            n_components: Number of principal components
            
        Returns:
            PCA analysis results
        """
        # Prepare data for PCA
        feature_matrix = []
        agent_names = []
        
        for agent_data in agents_data:
            features = [agent_data.get(metric, 0.0) for metric in metrics]
            feature_matrix.append(features)
            agent_names.append(agent_data.get('name', 'Unknown'))
        
        # Perform PCA
        pca = PCA(n_components=n_components)
        principal_components = pca.fit_transform(feature_matrix)
        
        # Create results
        results = {
            'agent_names': agent_names,
            'principal_components': principal_components.tolist(),
            'explained_variance_ratio': pca.explained_variance_ratio_.tolist(),
            'components': pca.components_.tolist(),
            'feature_names': metrics
        }
        
        return results
